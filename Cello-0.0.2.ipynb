{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cello 0.0.2\n",
        "\n",
        "Update : implemented attention mechanism for better prediction and visualization of inference.\n",
        "\n",
        "The training data was downloaded from [BindingDB](bindingdb.org ) provided by UCSD."
      ],
      "metadata": {
        "id": "WAQEab3wz6kO"
      },
      "id": "WAQEab3wz6kO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "AfGUHM9VNQdF"
      },
      "id": "AfGUHM9VNQdF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959d43c4",
      "metadata": {
        "id": "959d43c4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "qJXACWiFANos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c9d867-694c-4470-8f79-7388a5a918b2"
      },
      "id": "qJXACWiFANos",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3abff0",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "9a3abff0",
        "outputId": "bff8349d-f1f1-4076-9bd0-c3cb5800f2a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          Ligand SMILES  IC50 (nM)  \\\n",
              "0     CC(C)[C@H](NC(C)=O)C(=O)N[C@@H](Cc1ccccc1)[C@@...         85   \n",
              "1     CC(C)C[C@]1(N=CC([C@@H](Cc2ccccc2)C(N)=O)C1=O)...         10   \n",
              "2     CC(C)[C@H](NC(=O)c1ccc2ccccc2n1)C(=O)N[C@@H](C...         10   \n",
              "3              Cc1cc(NCN2C(=O)c3ccccc3C2=O)c(=O)[nH]c1C        218   \n",
              "4     CC(C)[C@H](NC(=O)[C@@H](C[C@H](O)[C@H](Cc1cccc...         11   \n",
              "...                                                 ...        ...   \n",
              "4737                 Cc1c(Br)cc(NC(=O)CS(O)(=O)=O)cc1Br         62   \n",
              "4738                 Cc1c(Br)cc(NC(=O)CS(O)(=O)=O)cc1Br         62   \n",
              "4739  NCCNS(=O)(=O)c1ccc(cc1)-c1ccc(CSc2nc3CCCc3c(=O...         83   \n",
              "4740  COC(=O)C[C@H]1[C@@]2(C)[C@H](C[C@@H]3CC(C(C)=C...        240   \n",
              "4741  Nc1nc(Cl)cc(n1)-c1nn(cc1Cc1ccccc1OCCN1CCOCC1)C...         19   \n",
              "\n",
              "                        BindingDB Target Chain Sequence  \n",
              "0     PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMNLPGRWKPKM...  \n",
              "1     PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...  \n",
              "2     PQFSLWKRPVVTAYIEGQPVEVLLDTGADDSIVAGIELGNNYSPKI...  \n",
              "3     PISPIETVPVKLKPGMDGPKVKQWPLTEEKIKALVEICTEMEKEGK...  \n",
              "4     PQVTLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...  \n",
              "...                                                 ...  \n",
              "4737  MAEVGSKSVLFVCLGNICRSPIAEAVFRKLVTDEKVSDNWRIDSAA...  \n",
              "4738  MAEVGSKSVLFVCLGNICRSPIAEAVFRKLVTDENVSDNWRIDSAA...  \n",
              "4739  MIRLGAPQTLVLLTLLVAAVLRCQGQDVQEAGSCVQDGQRYNDKDV...  \n",
              "4740  MAAQQRDCGGAAQLAGPAAEADPLGRFTCPVCLEVYEKPVQVPCGH...  \n",
              "4741  MSARRQELQDRAIVKIAAHLPDLIVYGDFSPERPSVKCFDGVLMFV...  \n",
              "\n",
              "[4742 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e84fde47-7310-4445-9081-474fe89d3ff2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ligand SMILES</th>\n",
              "      <th>IC50 (nM)</th>\n",
              "      <th>BindingDB Target Chain Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CC(C)[C@H](NC(C)=O)C(=O)N[C@@H](Cc1ccccc1)[C@@...</td>\n",
              "      <td>85</td>\n",
              "      <td>PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMNLPGRWKPKM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CC(C)C[C@]1(N=CC([C@@H](Cc2ccccc2)C(N)=O)C1=O)...</td>\n",
              "      <td>10</td>\n",
              "      <td>PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CC(C)[C@H](NC(=O)c1ccc2ccccc2n1)C(=O)N[C@@H](C...</td>\n",
              "      <td>10</td>\n",
              "      <td>PQFSLWKRPVVTAYIEGQPVEVLLDTGADDSIVAGIELGNNYSPKI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cc1cc(NCN2C(=O)c3ccccc3C2=O)c(=O)[nH]c1C</td>\n",
              "      <td>218</td>\n",
              "      <td>PISPIETVPVKLKPGMDGPKVKQWPLTEEKIKALVEICTEMEKEGK...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CC(C)[C@H](NC(=O)[C@@H](C[C@H](O)[C@H](Cc1cccc...</td>\n",
              "      <td>11</td>\n",
              "      <td>PQVTLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4737</th>\n",
              "      <td>Cc1c(Br)cc(NC(=O)CS(O)(=O)=O)cc1Br</td>\n",
              "      <td>62</td>\n",
              "      <td>MAEVGSKSVLFVCLGNICRSPIAEAVFRKLVTDEKVSDNWRIDSAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4738</th>\n",
              "      <td>Cc1c(Br)cc(NC(=O)CS(O)(=O)=O)cc1Br</td>\n",
              "      <td>62</td>\n",
              "      <td>MAEVGSKSVLFVCLGNICRSPIAEAVFRKLVTDENVSDNWRIDSAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4739</th>\n",
              "      <td>NCCNS(=O)(=O)c1ccc(cc1)-c1ccc(CSc2nc3CCCc3c(=O...</td>\n",
              "      <td>83</td>\n",
              "      <td>MIRLGAPQTLVLLTLLVAAVLRCQGQDVQEAGSCVQDGQRYNDKDV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4740</th>\n",
              "      <td>COC(=O)C[C@H]1[C@@]2(C)[C@H](C[C@@H]3CC(C(C)=C...</td>\n",
              "      <td>240</td>\n",
              "      <td>MAAQQRDCGGAAQLAGPAAEADPLGRFTCPVCLEVYEKPVQVPCGH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4741</th>\n",
              "      <td>Nc1nc(Cl)cc(n1)-c1nn(cc1Cc1ccccc1OCCN1CCOCC1)C...</td>\n",
              "      <td>19</td>\n",
              "      <td>MSARRQELQDRAIVKIAAHLPDLIVYGDFSPERPSVKCFDGVLMFV...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4742 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e84fde47-7310-4445-9081-474fe89d3ff2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e84fde47-7310-4445-9081-474fe89d3ff2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e84fde47-7310-4445-9081-474fe89d3ff2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dccf2c96-ac1f-4c73-b8a2-12526663dd87\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dccf2c96-ac1f-4c73-b8a2-12526663dd87')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dccf2c96-ac1f-4c73-b8a2-12526663dd87 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data_drop = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CPI project/data_drop.csv')\n",
        "data_drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d086d9f",
      "metadata": {
        "id": "2d086d9f"
      },
      "outputs": [],
      "source": [
        "# split into train, valid, test dataset and convert to iterable dataset type\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "_train = []\n",
        "for data_list in data_drop.values:\n",
        "  _train.append(data_list)\n",
        "\n",
        "num_train = int(len(_train) * 0.90)\n",
        "num_test = int(len(_train)*0.05)\n",
        "_train, _valid, _test = random_split(_train, [num_train, len(_train) - num_train - num_test, num_test])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepchem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inIxCHWPA6cl",
        "outputId": "db6ad108-d317-40a9-f49b-f715ade8e605"
      },
      "id": "inIxCHWPA6cl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Collecting scipy<1.9 (from deepchem)\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit (from deepchem)\n",
            "  Downloading rdkit-2023.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.3.post1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: scipy, rdkit, deepchem\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "jax 0.4.20 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\n",
            "jaxlib 0.4.20+cuda11.cudnn86 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deepchem-2.7.1 rdkit-2023.9.2 scipy-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize amino acid sequence and SMILES sequence"
      ],
      "metadata": {
        "id": "YbhIgFQMNnkk"
      },
      "id": "YbhIgFQMNnkk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53efb752",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53efb752",
        "outputId": "c251ddee-5b5f-4e9c-eb9b-5ffc47d42493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.10/dist-packages/deepchem/models/torch_models/__init__.py)\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = iter(_train)\n",
        "valid_iter = iter(_valid)\n",
        "test_iter = iter(_test)\n",
        "\n",
        "tokenize_smiles = BasicSmilesTokenizer()\n",
        "\n",
        "def yield_tokens(file_iter, type = 0):\n",
        "     if type == 0:\n",
        "        for line in file_iter:\n",
        "            smi_token = list(tokenize_smiles.tokenize(str(line[0])))\n",
        "            smi_token.append('<eos>')\n",
        "            smi_token.insert(0, '<sos>')\n",
        "            yield smi_token\n",
        "     else:\n",
        "        for line in file_iter:\n",
        "            aa_token = list(line[2])\n",
        "            aa_token.append('<eos>')\n",
        "            aa_token.insert(0, '<sos>')\n",
        "            yield aa_token\n",
        "\n",
        "smiles_voc = build_vocab_from_iterator(yield_tokens(train_iter, 0), specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "train_iter = iter(_train)\n",
        "aa_voc = build_vocab_from_iterator(yield_tokens(train_iter, 1), specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "smiles_voc.set_default_index(smiles_voc['<unk>'])\n",
        "aa_voc.set_default_index(aa_voc['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51f0e21d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51f0e21d",
        "outputId": "be010389-4843-49bf-edad-11166bdb6bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77\n",
            "29\n"
          ]
        }
      ],
      "source": [
        "print(len(smiles_voc))\n",
        "print(len(aa_voc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `<sos>` and `<eos>` token to start / end"
      ],
      "metadata": {
        "id": "QJD-GrPXNsN_"
      },
      "id": "QJD-GrPXNsN_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae358f8",
      "metadata": {
        "id": "1ae358f8"
      },
      "outputs": [],
      "source": [
        "def be(a_list):\n",
        "    a_list.append('<eos>')\n",
        "    a_list.insert(0, '<sos>')\n",
        "    return a_list\n",
        "\n",
        "smiles_pipeline = lambda x: smiles_voc(be(tokenize_smiles.tokenize(str(x))))\n",
        "aa_pipeline = lambda x: aa_voc(be(list(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dfb53b3",
      "metadata": {
        "id": "2dfb53b3"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09860d32",
      "metadata": {
        "id": "09860d32"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(batch):\n",
        "    aa_list, smiles_list, aa_len = [], [], []\n",
        "    for [_smi, _, _aa] in batch:\n",
        "         processed_smi = torch.tensor(smiles_pipeline(_smi), dtype=torch.int64)\n",
        "         smiles_list.append(processed_smi)\n",
        "         processed_aa = torch.tensor(aa_pipeline(_aa), dtype=torch.int64)\n",
        "         aa_list.append(processed_aa)\n",
        "         aa_len.append(len(processed_aa))\n",
        "    smiles_list = pad_sequence(smiles_list, padding_value = 1)\n",
        "    aa_list = pad_sequence(aa_list, padding_value = 1)\n",
        "\n",
        "    return aa_list.to(device), smiles_list.to(device), aa_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb32b762",
      "metadata": {
        "id": "cb32b762"
      },
      "outputs": [],
      "source": [
        "train_iter = iter(_train)\n",
        "valid_iter = iter(_valid)\n",
        "test_iter = iter(_test)\n",
        "\n",
        "train_dataloader = DataLoader(list(train_iter), batch_size=2,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "valid_dataloader = DataLoader(list(valid_iter), batch_size=2,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "test_dataloader = DataLoader(list(test_iter), batch_size=2,\n",
        "                              shuffle=True, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "- based on https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*qN2Pj5J4VqAFf7dsA2dHpA.png)"
      ],
      "metadata": {
        "id": "3-zauAIhN6LE"
      },
      "id": "3-zauAIhN6LE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b1aaff",
      "metadata": {
        "id": "85b1aaff"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "\n",
        "        self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, aa, aa_len):\n",
        "\n",
        "        #aa = [aa len, batch size]\n",
        "        #aa_len = [batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(aa))\n",
        "        #embedded = [aa len, batch size, emb dim]\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths = aa_len, enforce_sorted = False)\n",
        "\n",
        "\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "        #outputs = [aa len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d205b15c",
      "metadata": {
        "id": "d205b15c"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "\n",
        "        #hidden = [batch size, dec hid dim] from decoder\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2] from bidirectional encoder\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
        "        #energy = [batch size, src len, dec hid dim] #nn.Linear(dec_hid_dim, 1)\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "\n",
        "        #attention = [batch size, src len]\n",
        "\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        return nn.functional.softmax(attention, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4458fc61",
      "metadata": {
        "id": "4458fc61"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "\n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "\n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "\n",
        "        #a = [batch size, src len]\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        #a = [batch size, 1, src len]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "\n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "\n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "\n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "\n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "\n",
        "        #prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ccb969",
      "metadata": {
        "id": "a2ccb969"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, aa_pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.aa_pad_idx = aa_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def create_mask(self, aa):\n",
        "        mask = (aa != self.aa_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, aa, aa_len, smi, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        #aa = [aa len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #smi = [smi len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        if smi is None:\n",
        "            smi = torch.zeros((50, aa.shape[1])).fill_(2).long().to(aa.device)\n",
        "\n",
        "        batch_size = smi.shape[1]\n",
        "        smi_len = smi.shape[0]\n",
        "        smi_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(smi_len, batch_size, smi_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        encoder_outputs, hidden = self.encoder(aa, aa_len)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = smi[0,:]\n",
        "\n",
        "        mask = self.create_mask(aa)\n",
        "        #mask = [batch size, aa_len]\n",
        "\n",
        "        for t in range(1, smi_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden, all enconder hidden states, and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = smi[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b38f389",
      "metadata": {
        "id": "5b38f389"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(aa_voc)\n",
        "OUTPUT_DIM = len(smiles_voc)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "AA_PAD_IDX = aa_voc(['<pad>'])[0]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, AA_PAD_IDX, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c9d684",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13c9d684",
        "outputId": "9daaab84-d815-45be-9bda-35df4f81e185"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(29, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(77, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=77, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c5580b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16c5580b",
        "outputId": "67f8036e-f788-4a21-ff80-3ac7baafdf25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 6,598,477 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define our optimizer and criterion.\n",
        "\n",
        "The `ignore_index` for the criterion needs to be the index of the pad token for the target SMILES, not the source amino acid sequence."
      ],
      "metadata": {
        "id": "U3vsXJnMObU-"
      },
      "id": "U3vsXJnMObU-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003e6067",
      "metadata": {
        "id": "003e6067"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "281ea384",
      "metadata": {
        "id": "281ea384"
      },
      "outputs": [],
      "source": [
        "SMI_PAD_IDX = smiles_voc(['<pad>'])[0]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = SMI_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960d7a31",
      "metadata": {
        "id": "960d7a31"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "    log_interval = 10\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "        aa = batch[0]\n",
        "        smi = batch[1]\n",
        "        aa_len = batch[2]\n",
        "        batch_loss = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(aa, aa_len, smi)\n",
        "\n",
        "        #smi = [smi len, batch size]\n",
        "        #output = [smi len, batch size, output dim]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        smi = smi[1:].view(-1)\n",
        "\n",
        "        #smi = [(smi len - 1) * batch size]\n",
        "        #output = [(smi len - 1) * batch size, output dim]\n",
        "\n",
        "        loss = criterion(output, smi)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        batch_loss += loss.item()\n",
        "\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| loss {:8.2f}'.format(epoch+1, i, len(iterator),\n",
        "                                           batch_loss))\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48daac2c",
      "metadata": {
        "id": "48daac2c"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            aa = batch[0]\n",
        "            smi = batch[1]\n",
        "            aa_len = batch[2]\n",
        "\n",
        "            output = model(aa, aa_len, smi, 0) #turn off teacher forcing\n",
        "\n",
        "            #smi = [smi len, batch size]\n",
        "            #output = [smi len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            smi = smi[1:].view(-1)\n",
        "\n",
        "            #smi = [(smi len - 1) * batch size]\n",
        "            #output = [(smi len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, smi)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ac6780",
      "metadata": {
        "id": "f9ac6780"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The penultimate step is to train our model."
      ],
      "metadata": {
        "id": "hJBEIUPVOvfu"
      },
      "id": "hJBEIUPVOvfu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6667d03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c6667d03",
        "outputId": "fccd18d6-ca5f-4db6-f2af-bdbb6b122fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    10/ 2134 batches | loss     2.51\n",
            "| epoch   1 |    20/ 2134 batches | loss     2.80\n",
            "| epoch   1 |    30/ 2134 batches | loss     2.45\n",
            "| epoch   1 |    40/ 2134 batches | loss     2.39\n",
            "| epoch   1 |    50/ 2134 batches | loss     2.32\n",
            "| epoch   1 |    60/ 2134 batches | loss     2.54\n",
            "| epoch   1 |    70/ 2134 batches | loss     2.51\n",
            "| epoch   1 |    80/ 2134 batches | loss     2.48\n",
            "| epoch   1 |    90/ 2134 batches | loss     2.35\n",
            "| epoch   1 |   100/ 2134 batches | loss     2.66\n",
            "| epoch   1 |   110/ 2134 batches | loss     2.25\n",
            "| epoch   1 |   120/ 2134 batches | loss     2.80\n",
            "| epoch   1 |   130/ 2134 batches | loss     2.01\n",
            "| epoch   1 |   140/ 2134 batches | loss     1.93\n",
            "| epoch   1 |   150/ 2134 batches | loss     2.26\n",
            "| epoch   1 |   160/ 2134 batches | loss     2.41\n",
            "| epoch   1 |   170/ 2134 batches | loss     2.16\n",
            "| epoch   1 |   180/ 2134 batches | loss     2.02\n",
            "| epoch   1 |   190/ 2134 batches | loss     1.96\n",
            "| epoch   1 |   200/ 2134 batches | loss     1.94\n",
            "| epoch   1 |   210/ 2134 batches | loss     2.09\n",
            "| epoch   1 |   220/ 2134 batches | loss     2.13\n",
            "| epoch   1 |   230/ 2134 batches | loss     2.05\n",
            "| epoch   1 |   240/ 2134 batches | loss     2.76\n",
            "| epoch   1 |   250/ 2134 batches | loss     2.27\n",
            "| epoch   1 |   260/ 2134 batches | loss     2.37\n",
            "| epoch   1 |   270/ 2134 batches | loss     1.99\n",
            "| epoch   1 |   280/ 2134 batches | loss     2.02\n",
            "| epoch   1 |   290/ 2134 batches | loss     2.28\n",
            "| epoch   1 |   300/ 2134 batches | loss     1.82\n",
            "| epoch   1 |   310/ 2134 batches | loss     2.57\n",
            "| epoch   1 |   320/ 2134 batches | loss     2.23\n",
            "| epoch   1 |   330/ 2134 batches | loss     2.27\n",
            "| epoch   1 |   340/ 2134 batches | loss     2.59\n",
            "| epoch   1 |   350/ 2134 batches | loss     2.16\n",
            "| epoch   1 |   360/ 2134 batches | loss     1.76\n",
            "| epoch   1 |   370/ 2134 batches | loss     2.05\n",
            "| epoch   1 |   380/ 2134 batches | loss     2.32\n",
            "| epoch   1 |   390/ 2134 batches | loss     2.63\n",
            "| epoch   1 |   400/ 2134 batches | loss     2.05\n",
            "| epoch   1 |   410/ 2134 batches | loss     2.16\n",
            "| epoch   1 |   420/ 2134 batches | loss     2.22\n",
            "| epoch   1 |   430/ 2134 batches | loss     2.33\n",
            "| epoch   1 |   440/ 2134 batches | loss     2.22\n",
            "| epoch   1 |   450/ 2134 batches | loss     2.13\n",
            "| epoch   1 |   460/ 2134 batches | loss     2.38\n",
            "| epoch   1 |   470/ 2134 batches | loss     2.01\n",
            "| epoch   1 |   480/ 2134 batches | loss     2.44\n",
            "| epoch   1 |   490/ 2134 batches | loss     2.07\n",
            "| epoch   1 |   500/ 2134 batches | loss     2.07\n",
            "| epoch   1 |   510/ 2134 batches | loss     2.21\n",
            "| epoch   1 |   520/ 2134 batches | loss     2.21\n",
            "| epoch   1 |   530/ 2134 batches | loss     2.14\n",
            "| epoch   1 |   540/ 2134 batches | loss     2.16\n",
            "| epoch   1 |   550/ 2134 batches | loss     2.17\n",
            "| epoch   1 |   560/ 2134 batches | loss     1.97\n",
            "| epoch   1 |   570/ 2134 batches | loss     1.82\n",
            "| epoch   1 |   580/ 2134 batches | loss     1.93\n",
            "| epoch   1 |   590/ 2134 batches | loss     2.40\n",
            "| epoch   1 |   600/ 2134 batches | loss     2.24\n",
            "| epoch   1 |   610/ 2134 batches | loss     2.00\n",
            "| epoch   1 |   620/ 2134 batches | loss     1.92\n",
            "| epoch   1 |   630/ 2134 batches | loss     2.37\n",
            "| epoch   1 |   640/ 2134 batches | loss     2.05\n",
            "| epoch   1 |   650/ 2134 batches | loss     2.05\n",
            "| epoch   1 |   660/ 2134 batches | loss     2.06\n",
            "| epoch   1 |   670/ 2134 batches | loss     2.15\n",
            "| epoch   1 |   680/ 2134 batches | loss     2.38\n",
            "| epoch   1 |   690/ 2134 batches | loss     2.31\n",
            "| epoch   1 |   700/ 2134 batches | loss     1.97\n",
            "| epoch   1 |   710/ 2134 batches | loss     2.03\n",
            "| epoch   1 |   720/ 2134 batches | loss     2.07\n",
            "| epoch   1 |   730/ 2134 batches | loss     2.04\n",
            "| epoch   1 |   740/ 2134 batches | loss     2.18\n",
            "| epoch   1 |   750/ 2134 batches | loss     1.93\n",
            "| epoch   1 |   760/ 2134 batches | loss     2.18\n",
            "| epoch   1 |   770/ 2134 batches | loss     2.13\n",
            "| epoch   1 |   780/ 2134 batches | loss     1.97\n",
            "| epoch   1 |   790/ 2134 batches | loss     2.17\n",
            "| epoch   1 |   800/ 2134 batches | loss     2.24\n",
            "| epoch   1 |   810/ 2134 batches | loss     1.76\n",
            "| epoch   1 |   820/ 2134 batches | loss     1.91\n",
            "| epoch   1 |   830/ 2134 batches | loss     2.02\n",
            "| epoch   1 |   840/ 2134 batches | loss     2.13\n",
            "| epoch   1 |   850/ 2134 batches | loss     2.30\n",
            "| epoch   1 |   860/ 2134 batches | loss     2.01\n",
            "| epoch   1 |   870/ 2134 batches | loss     2.42\n",
            "| epoch   1 |   880/ 2134 batches | loss     1.93\n",
            "| epoch   1 |   890/ 2134 batches | loss     2.15\n",
            "| epoch   1 |   900/ 2134 batches | loss     2.13\n",
            "| epoch   1 |   910/ 2134 batches | loss     1.95\n",
            "| epoch   1 |   920/ 2134 batches | loss     2.12\n",
            "| epoch   1 |   930/ 2134 batches | loss     2.03\n",
            "| epoch   1 |   940/ 2134 batches | loss     1.71\n",
            "| epoch   1 |   950/ 2134 batches | loss     2.13\n",
            "| epoch   1 |   960/ 2134 batches | loss     1.87\n",
            "| epoch   1 |   970/ 2134 batches | loss     2.09\n",
            "| epoch   1 |   980/ 2134 batches | loss     2.24\n",
            "| epoch   1 |   990/ 2134 batches | loss     2.19\n",
            "| epoch   1 |  1000/ 2134 batches | loss     2.03\n",
            "| epoch   1 |  1010/ 2134 batches | loss     2.16\n",
            "| epoch   1 |  1020/ 2134 batches | loss     2.11\n",
            "| epoch   1 |  1030/ 2134 batches | loss     2.35\n",
            "| epoch   1 |  1040/ 2134 batches | loss     1.96\n",
            "| epoch   1 |  1050/ 2134 batches | loss     2.04\n",
            "| epoch   1 |  1060/ 2134 batches | loss     2.23\n",
            "| epoch   1 |  1070/ 2134 batches | loss     2.03\n",
            "| epoch   1 |  1080/ 2134 batches | loss     2.28\n",
            "| epoch   1 |  1090/ 2134 batches | loss     2.00\n",
            "| epoch   1 |  1100/ 2134 batches | loss     2.04\n",
            "| epoch   1 |  1110/ 2134 batches | loss     2.03\n",
            "| epoch   1 |  1120/ 2134 batches | loss     2.08\n",
            "| epoch   1 |  1130/ 2134 batches | loss     1.81\n",
            "| epoch   1 |  1140/ 2134 batches | loss     2.25\n",
            "| epoch   1 |  1150/ 2134 batches | loss     2.03\n",
            "| epoch   1 |  1160/ 2134 batches | loss     2.00\n",
            "| epoch   1 |  1170/ 2134 batches | loss     2.02\n",
            "| epoch   1 |  1180/ 2134 batches | loss     2.19\n",
            "| epoch   1 |  1190/ 2134 batches | loss     2.08\n",
            "| epoch   1 |  1200/ 2134 batches | loss     1.92\n",
            "| epoch   1 |  1210/ 2134 batches | loss     1.89\n",
            "| epoch   1 |  1220/ 2134 batches | loss     2.24\n",
            "| epoch   1 |  1230/ 2134 batches | loss     2.26\n",
            "| epoch   1 |  1240/ 2134 batches | loss     1.84\n",
            "| epoch   1 |  1250/ 2134 batches | loss     2.09\n",
            "| epoch   1 |  1260/ 2134 batches | loss     2.60\n",
            "| epoch   1 |  1270/ 2134 batches | loss     2.46\n",
            "| epoch   1 |  1280/ 2134 batches | loss     2.64\n",
            "| epoch   1 |  1290/ 2134 batches | loss     2.46\n",
            "| epoch   1 |  1300/ 2134 batches | loss     2.02\n",
            "| epoch   1 |  1310/ 2134 batches | loss     2.08\n",
            "| epoch   1 |  1320/ 2134 batches | loss     1.88\n",
            "| epoch   1 |  1330/ 2134 batches | loss     2.35\n",
            "| epoch   1 |  1340/ 2134 batches | loss     1.99\n",
            "| epoch   1 |  1350/ 2134 batches | loss     1.77\n",
            "| epoch   1 |  1360/ 2134 batches | loss     2.28\n",
            "| epoch   1 |  1370/ 2134 batches | loss     1.84\n",
            "| epoch   1 |  1380/ 2134 batches | loss     1.98\n",
            "| epoch   1 |  1390/ 2134 batches | loss     2.09\n",
            "| epoch   1 |  1400/ 2134 batches | loss     3.00\n",
            "| epoch   1 |  1410/ 2134 batches | loss     2.46\n",
            "| epoch   1 |  1420/ 2134 batches | loss     2.86\n",
            "| epoch   1 |  1430/ 2134 batches | loss     2.06\n",
            "| epoch   1 |  1440/ 2134 batches | loss     1.92\n",
            "| epoch   1 |  1450/ 2134 batches | loss     1.76\n",
            "| epoch   1 |  1460/ 2134 batches | loss     2.40\n",
            "| epoch   1 |  1470/ 2134 batches | loss     2.05\n",
            "| epoch   1 |  1480/ 2134 batches | loss     2.31\n",
            "| epoch   1 |  1490/ 2134 batches | loss     2.13\n",
            "| epoch   1 |  1500/ 2134 batches | loss     1.93\n",
            "| epoch   1 |  1510/ 2134 batches | loss     1.91\n",
            "| epoch   1 |  1520/ 2134 batches | loss     1.87\n",
            "| epoch   1 |  1530/ 2134 batches | loss     2.21\n",
            "| epoch   1 |  1540/ 2134 batches | loss     1.99\n",
            "| epoch   1 |  1550/ 2134 batches | loss     1.91\n",
            "| epoch   1 |  1560/ 2134 batches | loss     1.71\n",
            "| epoch   1 |  1570/ 2134 batches | loss     2.19\n",
            "| epoch   1 |  1580/ 2134 batches | loss     2.03\n",
            "| epoch   1 |  1590/ 2134 batches | loss     2.31\n",
            "| epoch   1 |  1600/ 2134 batches | loss     2.03\n",
            "| epoch   1 |  1610/ 2134 batches | loss     2.20\n",
            "| epoch   1 |  1620/ 2134 batches | loss     1.85\n",
            "| epoch   1 |  1630/ 2134 batches | loss     2.20\n",
            "| epoch   1 |  1640/ 2134 batches | loss     2.08\n",
            "| epoch   1 |  1650/ 2134 batches | loss     2.12\n",
            "| epoch   1 |  1660/ 2134 batches | loss     2.29\n",
            "| epoch   1 |  1670/ 2134 batches | loss     2.01\n",
            "| epoch   1 |  1680/ 2134 batches | loss     2.52\n",
            "| epoch   1 |  1690/ 2134 batches | loss     1.95\n",
            "| epoch   1 |  1700/ 2134 batches | loss     1.96\n",
            "| epoch   1 |  1710/ 2134 batches | loss     1.94\n",
            "| epoch   1 |  1720/ 2134 batches | loss     2.14\n",
            "| epoch   1 |  1730/ 2134 batches | loss     1.87\n",
            "| epoch   1 |  1740/ 2134 batches | loss     1.96\n",
            "| epoch   1 |  1750/ 2134 batches | loss     2.25\n",
            "| epoch   1 |  1760/ 2134 batches | loss     1.97\n",
            "| epoch   1 |  1770/ 2134 batches | loss     2.16\n",
            "| epoch   1 |  1780/ 2134 batches | loss     2.06\n",
            "| epoch   1 |  1790/ 2134 batches | loss     1.96\n",
            "| epoch   1 |  1800/ 2134 batches | loss     2.07\n",
            "| epoch   1 |  1810/ 2134 batches | loss     2.21\n",
            "| epoch   1 |  1820/ 2134 batches | loss     1.74\n",
            "| epoch   1 |  1830/ 2134 batches | loss     1.99\n",
            "| epoch   1 |  1840/ 2134 batches | loss     2.01\n",
            "| epoch   1 |  1850/ 2134 batches | loss     2.37\n",
            "| epoch   1 |  1860/ 2134 batches | loss     1.97\n",
            "| epoch   1 |  1870/ 2134 batches | loss     2.06\n",
            "| epoch   1 |  1880/ 2134 batches | loss     2.05\n",
            "| epoch   1 |  1890/ 2134 batches | loss     1.77\n",
            "| epoch   1 |  1900/ 2134 batches | loss     1.84\n",
            "| epoch   1 |  1910/ 2134 batches | loss     2.06\n",
            "| epoch   1 |  1920/ 2134 batches | loss     2.21\n",
            "| epoch   1 |  1930/ 2134 batches | loss     2.56\n",
            "| epoch   1 |  1940/ 2134 batches | loss     1.96\n",
            "| epoch   1 |  1950/ 2134 batches | loss     1.85\n",
            "| epoch   1 |  1960/ 2134 batches | loss     1.99\n",
            "| epoch   1 |  1970/ 2134 batches | loss     2.06\n",
            "| epoch   1 |  1980/ 2134 batches | loss     1.81\n",
            "| epoch   1 |  1990/ 2134 batches | loss     2.08\n",
            "| epoch   1 |  2000/ 2134 batches | loss     1.75\n",
            "| epoch   1 |  2010/ 2134 batches | loss     2.26\n",
            "| epoch   1 |  2020/ 2134 batches | loss     2.29\n",
            "| epoch   1 |  2030/ 2134 batches | loss     2.03\n",
            "| epoch   1 |  2040/ 2134 batches | loss     2.39\n",
            "| epoch   1 |  2050/ 2134 batches | loss     1.86\n",
            "| epoch   1 |  2060/ 2134 batches | loss     1.87\n",
            "| epoch   1 |  2070/ 2134 batches | loss     2.37\n",
            "| epoch   1 |  2080/ 2134 batches | loss     1.67\n",
            "| epoch   1 |  2090/ 2134 batches | loss     1.68\n",
            "| epoch   1 |  2100/ 2134 batches | loss     2.20\n",
            "| epoch   1 |  2110/ 2134 batches | loss     1.75\n",
            "| epoch   1 |  2120/ 2134 batches | loss     1.92\n",
            "| epoch   1 |  2130/ 2134 batches | loss     1.88\n",
            "Epoch: 01 | Time: 8m 35s\n",
            "\tTrain Loss: 2.149 | Train PPL:   8.576\n",
            "\t Val. Loss: 2.665 |  Val. PPL:  14.366\n",
            "| epoch   2 |    10/ 2134 batches | loss     2.15\n",
            "| epoch   2 |    20/ 2134 batches | loss     2.00\n",
            "| epoch   2 |    30/ 2134 batches | loss     1.90\n",
            "| epoch   2 |    40/ 2134 batches | loss     1.93\n",
            "| epoch   2 |    50/ 2134 batches | loss     2.05\n",
            "| epoch   2 |    60/ 2134 batches | loss     2.14\n",
            "| epoch   2 |    70/ 2134 batches | loss     2.24\n",
            "| epoch   2 |    80/ 2134 batches | loss     1.60\n",
            "| epoch   2 |    90/ 2134 batches | loss     1.68\n",
            "| epoch   2 |   100/ 2134 batches | loss     2.27\n",
            "| epoch   2 |   110/ 2134 batches | loss     2.19\n",
            "| epoch   2 |   120/ 2134 batches | loss     2.16\n",
            "| epoch   2 |   130/ 2134 batches | loss     1.88\n",
            "| epoch   2 |   140/ 2134 batches | loss     1.65\n",
            "| epoch   2 |   150/ 2134 batches | loss     1.85\n",
            "| epoch   2 |   160/ 2134 batches | loss     2.16\n",
            "| epoch   2 |   170/ 2134 batches | loss     2.53\n",
            "| epoch   2 |   180/ 2134 batches | loss     1.97\n",
            "| epoch   2 |   190/ 2134 batches | loss     1.76\n",
            "| epoch   2 |   200/ 2134 batches | loss     2.05\n",
            "| epoch   2 |   210/ 2134 batches | loss     2.19\n",
            "| epoch   2 |   220/ 2134 batches | loss     1.97\n",
            "| epoch   2 |   230/ 2134 batches | loss     1.78\n",
            "| epoch   2 |   240/ 2134 batches | loss     2.18\n",
            "| epoch   2 |   250/ 2134 batches | loss     2.15\n",
            "| epoch   2 |   260/ 2134 batches | loss     1.80\n",
            "| epoch   2 |   270/ 2134 batches | loss     1.97\n",
            "| epoch   2 |   280/ 2134 batches | loss     1.90\n",
            "| epoch   2 |   290/ 2134 batches | loss     2.27\n",
            "| epoch   2 |   300/ 2134 batches | loss     1.88\n",
            "| epoch   2 |   310/ 2134 batches | loss     2.00\n",
            "| epoch   2 |   320/ 2134 batches | loss     2.35\n",
            "| epoch   2 |   330/ 2134 batches | loss     1.98\n",
            "| epoch   2 |   340/ 2134 batches | loss     2.26\n",
            "| epoch   2 |   350/ 2134 batches | loss     2.01\n",
            "| epoch   2 |   360/ 2134 batches | loss     2.03\n",
            "| epoch   2 |   370/ 2134 batches | loss     1.59\n",
            "| epoch   2 |   380/ 2134 batches | loss     1.95\n",
            "| epoch   2 |   390/ 2134 batches | loss     1.84\n",
            "| epoch   2 |   400/ 2134 batches | loss     1.87\n",
            "| epoch   2 |   410/ 2134 batches | loss     2.20\n",
            "| epoch   2 |   420/ 2134 batches | loss     2.26\n",
            "| epoch   2 |   430/ 2134 batches | loss     1.83\n",
            "| epoch   2 |   440/ 2134 batches | loss     2.18\n",
            "| epoch   2 |   450/ 2134 batches | loss     1.74\n",
            "| epoch   2 |   460/ 2134 batches | loss     1.79\n",
            "| epoch   2 |   470/ 2134 batches | loss     1.95\n",
            "| epoch   2 |   480/ 2134 batches | loss     2.18\n",
            "| epoch   2 |   490/ 2134 batches | loss     2.15\n",
            "| epoch   2 |   500/ 2134 batches | loss     1.85\n",
            "| epoch   2 |   510/ 2134 batches | loss     1.98\n",
            "| epoch   2 |   520/ 2134 batches | loss     1.92\n",
            "| epoch   2 |   530/ 2134 batches | loss     2.36\n",
            "| epoch   2 |   540/ 2134 batches | loss     1.81\n",
            "| epoch   2 |   550/ 2134 batches | loss     2.33\n",
            "| epoch   2 |   560/ 2134 batches | loss     1.88\n",
            "| epoch   2 |   570/ 2134 batches | loss     1.74\n",
            "| epoch   2 |   580/ 2134 batches | loss     2.06\n",
            "| epoch   2 |   590/ 2134 batches | loss     2.17\n",
            "| epoch   2 |   600/ 2134 batches | loss     1.98\n",
            "| epoch   2 |   610/ 2134 batches | loss     2.05\n",
            "| epoch   2 |   620/ 2134 batches | loss     1.84\n",
            "| epoch   2 |   630/ 2134 batches | loss     2.30\n",
            "| epoch   2 |   640/ 2134 batches | loss     2.14\n",
            "| epoch   2 |   650/ 2134 batches | loss     1.81\n",
            "| epoch   2 |   660/ 2134 batches | loss     1.96\n",
            "| epoch   2 |   670/ 2134 batches | loss     2.18\n",
            "| epoch   2 |   680/ 2134 batches | loss     2.06\n",
            "| epoch   2 |   690/ 2134 batches | loss     2.02\n",
            "| epoch   2 |   700/ 2134 batches | loss     1.88\n",
            "| epoch   2 |   710/ 2134 batches | loss     1.73\n",
            "| epoch   2 |   720/ 2134 batches | loss     1.58\n",
            "| epoch   2 |   730/ 2134 batches | loss     1.71\n",
            "| epoch   2 |   740/ 2134 batches | loss     1.95\n",
            "| epoch   2 |   750/ 2134 batches | loss     1.50\n",
            "| epoch   2 |   760/ 2134 batches | loss     1.49\n",
            "| epoch   2 |   770/ 2134 batches | loss     1.94\n",
            "| epoch   2 |   780/ 2134 batches | loss     2.02\n",
            "| epoch   2 |   790/ 2134 batches | loss     2.33\n",
            "| epoch   2 |   800/ 2134 batches | loss     2.01\n",
            "| epoch   2 |   810/ 2134 batches | loss     2.12\n",
            "| epoch   2 |   820/ 2134 batches | loss     1.96\n",
            "| epoch   2 |   830/ 2134 batches | loss     1.68\n",
            "| epoch   2 |   840/ 2134 batches | loss     1.87\n",
            "| epoch   2 |   850/ 2134 batches | loss     1.71\n",
            "| epoch   2 |   860/ 2134 batches | loss     1.89\n",
            "| epoch   2 |   870/ 2134 batches | loss     1.51\n",
            "| epoch   2 |   880/ 2134 batches | loss     2.02\n",
            "| epoch   2 |   890/ 2134 batches | loss     2.39\n",
            "| epoch   2 |   900/ 2134 batches | loss     2.39\n",
            "| epoch   2 |   910/ 2134 batches | loss     1.67\n",
            "| epoch   2 |   920/ 2134 batches | loss     1.74\n",
            "| epoch   2 |   930/ 2134 batches | loss     1.80\n",
            "| epoch   2 |   940/ 2134 batches | loss     2.33\n",
            "| epoch   2 |   950/ 2134 batches | loss     1.60\n",
            "| epoch   2 |   960/ 2134 batches | loss     1.74\n",
            "| epoch   2 |   970/ 2134 batches | loss     1.99\n",
            "| epoch   2 |   980/ 2134 batches | loss     2.32\n",
            "| epoch   2 |   990/ 2134 batches | loss     1.82\n",
            "| epoch   2 |  1000/ 2134 batches | loss     1.74\n",
            "| epoch   2 |  1010/ 2134 batches | loss     2.06\n",
            "| epoch   2 |  1020/ 2134 batches | loss     1.99\n",
            "| epoch   2 |  1030/ 2134 batches | loss     2.14\n",
            "| epoch   2 |  1040/ 2134 batches | loss     1.91\n",
            "| epoch   2 |  1050/ 2134 batches | loss     1.91\n",
            "| epoch   2 |  1060/ 2134 batches | loss     2.31\n",
            "| epoch   2 |  1070/ 2134 batches | loss     1.89\n",
            "| epoch   2 |  1080/ 2134 batches | loss     1.87\n",
            "| epoch   2 |  1090/ 2134 batches | loss     2.24\n",
            "| epoch   2 |  1100/ 2134 batches | loss     2.08\n",
            "| epoch   2 |  1110/ 2134 batches | loss     2.12\n",
            "| epoch   2 |  1120/ 2134 batches | loss     1.75\n",
            "| epoch   2 |  1130/ 2134 batches | loss     2.29\n",
            "| epoch   2 |  1140/ 2134 batches | loss     1.86\n",
            "| epoch   2 |  1150/ 2134 batches | loss     1.80\n",
            "| epoch   2 |  1160/ 2134 batches | loss     1.79\n",
            "| epoch   2 |  1170/ 2134 batches | loss     1.75\n",
            "| epoch   2 |  1180/ 2134 batches | loss     1.80\n",
            "| epoch   2 |  1190/ 2134 batches | loss     1.73\n",
            "| epoch   2 |  1200/ 2134 batches | loss     1.98\n",
            "| epoch   2 |  1210/ 2134 batches | loss     2.09\n",
            "| epoch   2 |  1220/ 2134 batches | loss     1.85\n",
            "| epoch   2 |  1230/ 2134 batches | loss     1.86\n",
            "| epoch   2 |  1240/ 2134 batches | loss     2.24\n",
            "| epoch   2 |  1250/ 2134 batches | loss     1.85\n",
            "| epoch   2 |  1260/ 2134 batches | loss     1.84\n",
            "| epoch   2 |  1270/ 2134 batches | loss     1.94\n",
            "| epoch   2 |  1280/ 2134 batches | loss     2.05\n",
            "| epoch   2 |  1290/ 2134 batches | loss     2.07\n",
            "| epoch   2 |  1300/ 2134 batches | loss     1.88\n",
            "| epoch   2 |  1310/ 2134 batches | loss     2.14\n",
            "| epoch   2 |  1320/ 2134 batches | loss     1.83\n",
            "| epoch   2 |  1330/ 2134 batches | loss     1.58\n",
            "| epoch   2 |  1340/ 2134 batches | loss     2.08\n",
            "| epoch   2 |  1350/ 2134 batches | loss     1.86\n",
            "| epoch   2 |  1360/ 2134 batches | loss     1.36\n",
            "| epoch   2 |  1370/ 2134 batches | loss     2.10\n",
            "| epoch   2 |  1380/ 2134 batches | loss     1.61\n",
            "| epoch   2 |  1390/ 2134 batches | loss     1.74\n",
            "| epoch   2 |  1400/ 2134 batches | loss     1.70\n",
            "| epoch   2 |  1410/ 2134 batches | loss     1.90\n",
            "| epoch   2 |  1420/ 2134 batches | loss     1.78\n",
            "| epoch   2 |  1430/ 2134 batches | loss     1.88\n",
            "| epoch   2 |  1440/ 2134 batches | loss     2.08\n",
            "| epoch   2 |  1450/ 2134 batches | loss     1.67\n",
            "| epoch   2 |  1460/ 2134 batches | loss     2.08\n",
            "| epoch   2 |  1470/ 2134 batches | loss     1.98\n",
            "| epoch   2 |  1480/ 2134 batches | loss     1.95\n",
            "| epoch   2 |  1490/ 2134 batches | loss     1.63\n",
            "| epoch   2 |  1500/ 2134 batches | loss     1.90\n",
            "| epoch   2 |  1510/ 2134 batches | loss     1.96\n",
            "| epoch   2 |  1520/ 2134 batches | loss     2.12\n",
            "| epoch   2 |  1530/ 2134 batches | loss     2.03\n",
            "| epoch   2 |  1540/ 2134 batches | loss     2.36\n",
            "| epoch   2 |  1550/ 2134 batches | loss     1.71\n",
            "| epoch   2 |  1560/ 2134 batches | loss     1.68\n",
            "| epoch   2 |  1570/ 2134 batches | loss     2.09\n",
            "| epoch   2 |  1580/ 2134 batches | loss     1.86\n",
            "| epoch   2 |  1590/ 2134 batches | loss     2.25\n",
            "| epoch   2 |  1600/ 2134 batches | loss     1.76\n",
            "| epoch   2 |  1610/ 2134 batches | loss     1.86\n",
            "| epoch   2 |  1620/ 2134 batches | loss     1.90\n",
            "| epoch   2 |  1630/ 2134 batches | loss     1.52\n",
            "| epoch   2 |  1640/ 2134 batches | loss     1.94\n",
            "| epoch   2 |  1650/ 2134 batches | loss     1.89\n",
            "| epoch   2 |  1660/ 2134 batches | loss     1.96\n",
            "| epoch   2 |  1670/ 2134 batches | loss     1.88\n",
            "| epoch   2 |  1680/ 2134 batches | loss     2.13\n",
            "| epoch   2 |  1690/ 2134 batches | loss     1.89\n",
            "| epoch   2 |  1700/ 2134 batches | loss     2.20\n",
            "| epoch   2 |  1710/ 2134 batches | loss     2.20\n",
            "| epoch   2 |  1720/ 2134 batches | loss     1.45\n",
            "| epoch   2 |  1730/ 2134 batches | loss     1.86\n",
            "| epoch   2 |  1740/ 2134 batches | loss     1.82\n",
            "| epoch   2 |  1750/ 2134 batches | loss     1.84\n",
            "| epoch   2 |  1760/ 2134 batches | loss     2.03\n",
            "| epoch   2 |  1770/ 2134 batches | loss     1.55\n",
            "| epoch   2 |  1780/ 2134 batches | loss     1.69\n",
            "| epoch   2 |  1790/ 2134 batches | loss     1.90\n",
            "| epoch   2 |  1800/ 2134 batches | loss     2.12\n",
            "| epoch   2 |  1810/ 2134 batches | loss     2.00\n",
            "| epoch   2 |  1820/ 2134 batches | loss     1.96\n",
            "| epoch   2 |  1830/ 2134 batches | loss     1.81\n",
            "| epoch   2 |  1840/ 2134 batches | loss     1.68\n",
            "| epoch   2 |  1850/ 2134 batches | loss     2.03\n",
            "| epoch   2 |  1860/ 2134 batches | loss     2.37\n",
            "| epoch   2 |  1870/ 2134 batches | loss     1.91\n",
            "| epoch   2 |  1880/ 2134 batches | loss     1.90\n",
            "| epoch   2 |  1890/ 2134 batches | loss     1.82\n",
            "| epoch   2 |  1900/ 2134 batches | loss     1.63\n",
            "| epoch   2 |  1910/ 2134 batches | loss     1.82\n",
            "| epoch   2 |  1920/ 2134 batches | loss     2.40\n",
            "| epoch   2 |  1930/ 2134 batches | loss     2.04\n",
            "| epoch   2 |  1940/ 2134 batches | loss     1.94\n",
            "| epoch   2 |  1950/ 2134 batches | loss     2.01\n",
            "| epoch   2 |  1960/ 2134 batches | loss     2.38\n",
            "| epoch   2 |  1970/ 2134 batches | loss     1.89\n",
            "| epoch   2 |  1980/ 2134 batches | loss     1.52\n",
            "| epoch   2 |  1990/ 2134 batches | loss     2.25\n",
            "| epoch   2 |  2000/ 2134 batches | loss     1.59\n",
            "| epoch   2 |  2010/ 2134 batches | loss     1.71\n",
            "| epoch   2 |  2020/ 2134 batches | loss     1.90\n",
            "| epoch   2 |  2030/ 2134 batches | loss     2.05\n",
            "| epoch   2 |  2040/ 2134 batches | loss     1.74\n",
            "| epoch   2 |  2050/ 2134 batches | loss     2.04\n",
            "| epoch   2 |  2060/ 2134 batches | loss     1.70\n",
            "| epoch   2 |  2070/ 2134 batches | loss     1.86\n",
            "| epoch   2 |  2080/ 2134 batches | loss     2.21\n",
            "| epoch   2 |  2090/ 2134 batches | loss     2.15\n",
            "| epoch   2 |  2100/ 2134 batches | loss     1.76\n",
            "| epoch   2 |  2110/ 2134 batches | loss     1.75\n",
            "| epoch   2 |  2120/ 2134 batches | loss     1.50\n",
            "| epoch   2 |  2130/ 2134 batches | loss     1.60\n",
            "Epoch: 02 | Time: 8m 37s\n",
            "\tTrain Loss: 1.964 | Train PPL:   7.128\n",
            "\t Val. Loss: 2.930 |  Val. PPL:  18.735\n",
            "| epoch   3 |    10/ 2134 batches | loss     2.06\n",
            "| epoch   3 |    20/ 2134 batches | loss     1.67\n",
            "| epoch   3 |    30/ 2134 batches | loss     1.94\n",
            "| epoch   3 |    40/ 2134 batches | loss     1.98\n",
            "| epoch   3 |    50/ 2134 batches | loss     1.92\n",
            "| epoch   3 |    60/ 2134 batches | loss     2.13\n",
            "| epoch   3 |    70/ 2134 batches | loss     1.56\n",
            "| epoch   3 |    80/ 2134 batches | loss     1.85\n",
            "| epoch   3 |    90/ 2134 batches | loss     1.68\n",
            "| epoch   3 |   100/ 2134 batches | loss     1.83\n",
            "| epoch   3 |   110/ 2134 batches | loss     1.71\n",
            "| epoch   3 |   120/ 2134 batches | loss     1.85\n",
            "| epoch   3 |   130/ 2134 batches | loss     2.05\n",
            "| epoch   3 |   140/ 2134 batches | loss     1.94\n",
            "| epoch   3 |   150/ 2134 batches | loss     2.10\n",
            "| epoch   3 |   160/ 2134 batches | loss     1.80\n",
            "| epoch   3 |   170/ 2134 batches | loss     2.02\n",
            "| epoch   3 |   180/ 2134 batches | loss     1.79\n",
            "| epoch   3 |   190/ 2134 batches | loss     2.05\n",
            "| epoch   3 |   200/ 2134 batches | loss     1.66\n",
            "| epoch   3 |   210/ 2134 batches | loss     1.86\n",
            "| epoch   3 |   220/ 2134 batches | loss     2.18\n",
            "| epoch   3 |   230/ 2134 batches | loss     1.74\n",
            "| epoch   3 |   240/ 2134 batches | loss     2.11\n",
            "| epoch   3 |   250/ 2134 batches | loss     2.09\n",
            "| epoch   3 |   260/ 2134 batches | loss     2.32\n",
            "| epoch   3 |   270/ 2134 batches | loss     2.15\n",
            "| epoch   3 |   280/ 2134 batches | loss     1.72\n",
            "| epoch   3 |   290/ 2134 batches | loss     2.29\n",
            "| epoch   3 |   300/ 2134 batches | loss     2.28\n",
            "| epoch   3 |   310/ 2134 batches | loss     2.01\n",
            "| epoch   3 |   320/ 2134 batches | loss     1.95\n",
            "| epoch   3 |   330/ 2134 batches | loss     1.82\n",
            "| epoch   3 |   340/ 2134 batches | loss     1.71\n",
            "| epoch   3 |   350/ 2134 batches | loss     2.06\n",
            "| epoch   3 |   360/ 2134 batches | loss     1.81\n",
            "| epoch   3 |   370/ 2134 batches | loss     2.08\n",
            "| epoch   3 |   380/ 2134 batches | loss     2.42\n",
            "| epoch   3 |   390/ 2134 batches | loss     1.82\n",
            "| epoch   3 |   400/ 2134 batches | loss     1.89\n",
            "| epoch   3 |   410/ 2134 batches | loss     1.82\n",
            "| epoch   3 |   420/ 2134 batches | loss     1.69\n",
            "| epoch   3 |   430/ 2134 batches | loss     1.55\n",
            "| epoch   3 |   440/ 2134 batches | loss     1.86\n",
            "| epoch   3 |   450/ 2134 batches | loss     1.88\n",
            "| epoch   3 |   460/ 2134 batches | loss     2.03\n",
            "| epoch   3 |   470/ 2134 batches | loss     2.33\n",
            "| epoch   3 |   480/ 2134 batches | loss     1.71\n",
            "| epoch   3 |   490/ 2134 batches | loss     1.57\n",
            "| epoch   3 |   500/ 2134 batches | loss     2.32\n",
            "| epoch   3 |   510/ 2134 batches | loss     1.77\n",
            "| epoch   3 |   520/ 2134 batches | loss     1.91\n",
            "| epoch   3 |   530/ 2134 batches | loss     2.07\n",
            "| epoch   3 |   540/ 2134 batches | loss     2.32\n",
            "| epoch   3 |   550/ 2134 batches | loss     2.05\n",
            "| epoch   3 |   560/ 2134 batches | loss     2.09\n",
            "| epoch   3 |   570/ 2134 batches | loss     1.70\n",
            "| epoch   3 |   580/ 2134 batches | loss     1.76\n",
            "| epoch   3 |   590/ 2134 batches | loss     2.11\n",
            "| epoch   3 |   600/ 2134 batches | loss     1.97\n",
            "| epoch   3 |   610/ 2134 batches | loss     2.47\n",
            "| epoch   3 |   620/ 2134 batches | loss     2.12\n",
            "| epoch   3 |   630/ 2134 batches | loss     1.68\n",
            "| epoch   3 |   640/ 2134 batches | loss     2.07\n",
            "| epoch   3 |   650/ 2134 batches | loss     1.48\n",
            "| epoch   3 |   660/ 2134 batches | loss     1.77\n",
            "| epoch   3 |   670/ 2134 batches | loss     1.95\n",
            "| epoch   3 |   680/ 2134 batches | loss     2.07\n",
            "| epoch   3 |   690/ 2134 batches | loss     1.80\n",
            "| epoch   3 |   700/ 2134 batches | loss     1.89\n",
            "| epoch   3 |   710/ 2134 batches | loss     1.88\n",
            "| epoch   3 |   720/ 2134 batches | loss     2.16\n",
            "| epoch   3 |   730/ 2134 batches | loss     2.25\n",
            "| epoch   3 |   740/ 2134 batches | loss     1.67\n",
            "| epoch   3 |   750/ 2134 batches | loss     2.19\n",
            "| epoch   3 |   760/ 2134 batches | loss     2.09\n",
            "| epoch   3 |   770/ 2134 batches | loss     1.93\n",
            "| epoch   3 |   780/ 2134 batches | loss     1.98\n",
            "| epoch   3 |   790/ 2134 batches | loss     1.52\n",
            "| epoch   3 |   800/ 2134 batches | loss     2.47\n",
            "| epoch   3 |   810/ 2134 batches | loss     1.77\n",
            "| epoch   3 |   820/ 2134 batches | loss     1.92\n",
            "| epoch   3 |   830/ 2134 batches | loss     1.62\n",
            "| epoch   3 |   840/ 2134 batches | loss     1.94\n",
            "| epoch   3 |   850/ 2134 batches | loss     2.11\n",
            "| epoch   3 |   860/ 2134 batches | loss     1.63\n",
            "| epoch   3 |   870/ 2134 batches | loss     2.11\n",
            "| epoch   3 |   880/ 2134 batches | loss     2.09\n",
            "| epoch   3 |   890/ 2134 batches | loss     2.00\n",
            "| epoch   3 |   900/ 2134 batches | loss     2.11\n",
            "| epoch   3 |   910/ 2134 batches | loss     2.07\n",
            "| epoch   3 |   920/ 2134 batches | loss     2.02\n",
            "| epoch   3 |   930/ 2134 batches | loss     1.36\n",
            "| epoch   3 |   940/ 2134 batches | loss     1.72\n",
            "| epoch   3 |   950/ 2134 batches | loss     2.13\n",
            "| epoch   3 |   960/ 2134 batches | loss     1.78\n",
            "| epoch   3 |   970/ 2134 batches | loss     1.93\n",
            "| epoch   3 |   980/ 2134 batches | loss     2.15\n",
            "| epoch   3 |   990/ 2134 batches | loss     1.80\n",
            "| epoch   3 |  1000/ 2134 batches | loss     1.70\n",
            "| epoch   3 |  1010/ 2134 batches | loss     2.09\n",
            "| epoch   3 |  1020/ 2134 batches | loss     1.89\n",
            "| epoch   3 |  1030/ 2134 batches | loss     2.07\n",
            "| epoch   3 |  1040/ 2134 batches | loss     2.06\n",
            "| epoch   3 |  1050/ 2134 batches | loss     1.98\n",
            "| epoch   3 |  1060/ 2134 batches | loss     1.64\n",
            "| epoch   3 |  1070/ 2134 batches | loss     2.01\n",
            "| epoch   3 |  1080/ 2134 batches | loss     1.58\n",
            "| epoch   3 |  1090/ 2134 batches | loss     2.11\n",
            "| epoch   3 |  1100/ 2134 batches | loss     2.30\n",
            "| epoch   3 |  1110/ 2134 batches | loss     1.61\n",
            "| epoch   3 |  1120/ 2134 batches | loss     1.93\n",
            "| epoch   3 |  1130/ 2134 batches | loss     1.73\n",
            "| epoch   3 |  1140/ 2134 batches | loss     1.63\n",
            "| epoch   3 |  1150/ 2134 batches | loss     1.52\n",
            "| epoch   3 |  1160/ 2134 batches | loss     1.61\n",
            "| epoch   3 |  1170/ 2134 batches | loss     1.78\n",
            "| epoch   3 |  1180/ 2134 batches | loss     2.24\n",
            "| epoch   3 |  1190/ 2134 batches | loss     1.79\n",
            "| epoch   3 |  1200/ 2134 batches | loss     1.62\n",
            "| epoch   3 |  1210/ 2134 batches | loss     1.68\n",
            "| epoch   3 |  1220/ 2134 batches | loss     2.13\n",
            "| epoch   3 |  1230/ 2134 batches | loss     1.95\n",
            "| epoch   3 |  1240/ 2134 batches | loss     1.90\n",
            "| epoch   3 |  1250/ 2134 batches | loss     1.99\n",
            "| epoch   3 |  1260/ 2134 batches | loss     2.01\n",
            "| epoch   3 |  1270/ 2134 batches | loss     1.80\n",
            "| epoch   3 |  1280/ 2134 batches | loss     1.89\n",
            "| epoch   3 |  1290/ 2134 batches | loss     1.88\n",
            "| epoch   3 |  1300/ 2134 batches | loss     1.61\n",
            "| epoch   3 |  1310/ 2134 batches | loss     2.09\n",
            "| epoch   3 |  1320/ 2134 batches | loss     1.90\n",
            "| epoch   3 |  1330/ 2134 batches | loss     1.92\n",
            "| epoch   3 |  1340/ 2134 batches | loss     1.97\n",
            "| epoch   3 |  1350/ 2134 batches | loss     1.93\n",
            "| epoch   3 |  1360/ 2134 batches | loss     1.93\n",
            "| epoch   3 |  1370/ 2134 batches | loss     1.57\n",
            "| epoch   3 |  1380/ 2134 batches | loss     2.03\n",
            "| epoch   3 |  1390/ 2134 batches | loss     1.90\n",
            "| epoch   3 |  1400/ 2134 batches | loss     1.89\n",
            "| epoch   3 |  1410/ 2134 batches | loss     1.75\n",
            "| epoch   3 |  1420/ 2134 batches | loss     1.75\n",
            "| epoch   3 |  1430/ 2134 batches | loss     1.84\n",
            "| epoch   3 |  1440/ 2134 batches | loss     2.00\n",
            "| epoch   3 |  1450/ 2134 batches | loss     2.24\n",
            "| epoch   3 |  1460/ 2134 batches | loss     1.97\n",
            "| epoch   3 |  1470/ 2134 batches | loss     1.94\n",
            "| epoch   3 |  1480/ 2134 batches | loss     1.96\n",
            "| epoch   3 |  1490/ 2134 batches | loss     1.77\n",
            "| epoch   3 |  1500/ 2134 batches | loss     1.42\n",
            "| epoch   3 |  1510/ 2134 batches | loss     1.73\n",
            "| epoch   3 |  1520/ 2134 batches | loss     1.57\n",
            "| epoch   3 |  1530/ 2134 batches | loss     1.59\n",
            "| epoch   3 |  1540/ 2134 batches | loss     1.43\n",
            "| epoch   3 |  1550/ 2134 batches | loss     1.77\n",
            "| epoch   3 |  1560/ 2134 batches | loss     1.89\n",
            "| epoch   3 |  1570/ 2134 batches | loss     2.15\n",
            "| epoch   3 |  1580/ 2134 batches | loss     2.00\n",
            "| epoch   3 |  1590/ 2134 batches | loss     1.69\n",
            "| epoch   3 |  1600/ 2134 batches | loss     1.88\n",
            "| epoch   3 |  1610/ 2134 batches | loss     1.99\n",
            "| epoch   3 |  1620/ 2134 batches | loss     1.64\n",
            "| epoch   3 |  1630/ 2134 batches | loss     1.73\n",
            "| epoch   3 |  1640/ 2134 batches | loss     2.17\n",
            "| epoch   3 |  1650/ 2134 batches | loss     1.82\n",
            "| epoch   3 |  1660/ 2134 batches | loss     1.89\n",
            "| epoch   3 |  1670/ 2134 batches | loss     2.20\n",
            "| epoch   3 |  1680/ 2134 batches | loss     1.81\n",
            "| epoch   3 |  1690/ 2134 batches | loss     1.93\n",
            "| epoch   3 |  1700/ 2134 batches | loss     1.84\n",
            "| epoch   3 |  1710/ 2134 batches | loss     1.79\n",
            "| epoch   3 |  1720/ 2134 batches | loss     2.12\n",
            "| epoch   3 |  1730/ 2134 batches | loss     2.01\n",
            "| epoch   3 |  1740/ 2134 batches | loss     1.68\n",
            "| epoch   3 |  1750/ 2134 batches | loss     1.55\n",
            "| epoch   3 |  1760/ 2134 batches | loss     1.74\n",
            "| epoch   3 |  1770/ 2134 batches | loss     2.28\n",
            "| epoch   3 |  1780/ 2134 batches | loss     1.87\n",
            "| epoch   3 |  1790/ 2134 batches | loss     2.02\n",
            "| epoch   3 |  1800/ 2134 batches | loss     2.34\n",
            "| epoch   3 |  1810/ 2134 batches | loss     2.20\n",
            "| epoch   3 |  1820/ 2134 batches | loss     1.90\n",
            "| epoch   3 |  1830/ 2134 batches | loss     1.87\n",
            "| epoch   3 |  1840/ 2134 batches | loss     1.65\n",
            "| epoch   3 |  1850/ 2134 batches | loss     1.70\n",
            "| epoch   3 |  1860/ 2134 batches | loss     2.08\n",
            "| epoch   3 |  1870/ 2134 batches | loss     1.66\n",
            "| epoch   3 |  1880/ 2134 batches | loss     1.76\n",
            "| epoch   3 |  1890/ 2134 batches | loss     1.41\n",
            "| epoch   3 |  1900/ 2134 batches | loss     1.53\n",
            "| epoch   3 |  1910/ 2134 batches | loss     1.98\n",
            "| epoch   3 |  1920/ 2134 batches | loss     1.64\n",
            "| epoch   3 |  1930/ 2134 batches | loss     1.91\n",
            "| epoch   3 |  1940/ 2134 batches | loss     2.06\n",
            "| epoch   3 |  1950/ 2134 batches | loss     1.64\n",
            "| epoch   3 |  1960/ 2134 batches | loss     1.70\n",
            "| epoch   3 |  1970/ 2134 batches | loss     2.07\n",
            "| epoch   3 |  1980/ 2134 batches | loss     2.31\n",
            "| epoch   3 |  1990/ 2134 batches | loss     1.89\n",
            "| epoch   3 |  2000/ 2134 batches | loss     1.49\n",
            "| epoch   3 |  2010/ 2134 batches | loss     1.86\n",
            "| epoch   3 |  2020/ 2134 batches | loss     1.75\n",
            "| epoch   3 |  2030/ 2134 batches | loss     1.97\n",
            "| epoch   3 |  2040/ 2134 batches | loss     1.77\n",
            "| epoch   3 |  2050/ 2134 batches | loss     1.55\n",
            "| epoch   3 |  2060/ 2134 batches | loss     2.02\n",
            "| epoch   3 |  2070/ 2134 batches | loss     1.68\n",
            "| epoch   3 |  2080/ 2134 batches | loss     2.28\n",
            "| epoch   3 |  2090/ 2134 batches | loss     1.92\n",
            "| epoch   3 |  2100/ 2134 batches | loss     2.12\n",
            "| epoch   3 |  2110/ 2134 batches | loss     2.19\n",
            "| epoch   3 |  2120/ 2134 batches | loss     1.82\n",
            "| epoch   3 |  2130/ 2134 batches | loss     1.75\n",
            "Epoch: 03 | Time: 8m 42s\n",
            "\tTrain Loss: 1.895 | Train PPL:   6.654\n",
            "\t Val. Loss: 2.692 |  Val. PPL:  14.764\n",
            "| epoch   4 |    10/ 2134 batches | loss     2.08\n",
            "| epoch   4 |    20/ 2134 batches | loss     1.71\n",
            "| epoch   4 |    30/ 2134 batches | loss     1.69\n",
            "| epoch   4 |    40/ 2134 batches | loss     1.70\n",
            "| epoch   4 |    50/ 2134 batches | loss     1.68\n",
            "| epoch   4 |    60/ 2134 batches | loss     1.97\n",
            "| epoch   4 |    70/ 2134 batches | loss     1.96\n",
            "| epoch   4 |    80/ 2134 batches | loss     1.84\n",
            "| epoch   4 |    90/ 2134 batches | loss     1.44\n",
            "| epoch   4 |   100/ 2134 batches | loss     1.76\n",
            "| epoch   4 |   110/ 2134 batches | loss     2.06\n",
            "| epoch   4 |   120/ 2134 batches | loss     1.57\n",
            "| epoch   4 |   130/ 2134 batches | loss     1.65\n",
            "| epoch   4 |   140/ 2134 batches | loss     1.93\n",
            "| epoch   4 |   150/ 2134 batches | loss     2.14\n",
            "| epoch   4 |   160/ 2134 batches | loss     1.72\n",
            "| epoch   4 |   170/ 2134 batches | loss     2.09\n",
            "| epoch   4 |   180/ 2134 batches | loss     1.47\n",
            "| epoch   4 |   190/ 2134 batches | loss     1.91\n",
            "| epoch   4 |   200/ 2134 batches | loss     1.91\n",
            "| epoch   4 |   210/ 2134 batches | loss     1.78\n",
            "| epoch   4 |   220/ 2134 batches | loss     1.39\n",
            "| epoch   4 |   230/ 2134 batches | loss     2.13\n",
            "| epoch   4 |   240/ 2134 batches | loss     1.99\n",
            "| epoch   4 |   250/ 2134 batches | loss     1.94\n",
            "| epoch   4 |   260/ 2134 batches | loss     1.98\n",
            "| epoch   4 |   270/ 2134 batches | loss     1.84\n",
            "| epoch   4 |   280/ 2134 batches | loss     1.87\n",
            "| epoch   4 |   290/ 2134 batches | loss     1.70\n",
            "| epoch   4 |   300/ 2134 batches | loss     2.02\n",
            "| epoch   4 |   310/ 2134 batches | loss     1.67\n",
            "| epoch   4 |   320/ 2134 batches | loss     1.79\n",
            "| epoch   4 |   330/ 2134 batches | loss     1.91\n",
            "| epoch   4 |   340/ 2134 batches | loss     1.47\n",
            "| epoch   4 |   350/ 2134 batches | loss     1.45\n",
            "| epoch   4 |   360/ 2134 batches | loss     1.69\n",
            "| epoch   4 |   370/ 2134 batches | loss     1.92\n",
            "| epoch   4 |   380/ 2134 batches | loss     1.90\n",
            "| epoch   4 |   390/ 2134 batches | loss     1.88\n",
            "| epoch   4 |   400/ 2134 batches | loss     1.82\n",
            "| epoch   4 |   410/ 2134 batches | loss     2.07\n",
            "| epoch   4 |   420/ 2134 batches | loss     2.22\n",
            "| epoch   4 |   430/ 2134 batches | loss     1.87\n",
            "| epoch   4 |   440/ 2134 batches | loss     1.41\n",
            "| epoch   4 |   450/ 2134 batches | loss     1.85\n",
            "| epoch   4 |   460/ 2134 batches | loss     1.60\n",
            "| epoch   4 |   470/ 2134 batches | loss     1.39\n",
            "| epoch   4 |   480/ 2134 batches | loss     1.51\n",
            "| epoch   4 |   490/ 2134 batches | loss     2.30\n",
            "| epoch   4 |   500/ 2134 batches | loss     1.58\n",
            "| epoch   4 |   510/ 2134 batches | loss     1.57\n",
            "| epoch   4 |   520/ 2134 batches | loss     2.22\n",
            "| epoch   4 |   530/ 2134 batches | loss     2.24\n",
            "| epoch   4 |   540/ 2134 batches | loss     2.00\n",
            "| epoch   4 |   550/ 2134 batches | loss     1.83\n",
            "| epoch   4 |   560/ 2134 batches | loss     2.14\n",
            "| epoch   4 |   570/ 2134 batches | loss     2.11\n",
            "| epoch   4 |   580/ 2134 batches | loss     2.17\n",
            "| epoch   4 |   590/ 2134 batches | loss     2.03\n",
            "| epoch   4 |   600/ 2134 batches | loss     1.95\n",
            "| epoch   4 |   610/ 2134 batches | loss     1.87\n",
            "| epoch   4 |   620/ 2134 batches | loss     2.18\n",
            "| epoch   4 |   630/ 2134 batches | loss     1.79\n",
            "| epoch   4 |   640/ 2134 batches | loss     1.77\n",
            "| epoch   4 |   650/ 2134 batches | loss     1.57\n",
            "| epoch   4 |   660/ 2134 batches | loss     1.95\n",
            "| epoch   4 |   670/ 2134 batches | loss     2.03\n",
            "| epoch   4 |   680/ 2134 batches | loss     1.80\n",
            "| epoch   4 |   690/ 2134 batches | loss     1.80\n",
            "| epoch   4 |   700/ 2134 batches | loss     1.96\n",
            "| epoch   4 |   710/ 2134 batches | loss     1.98\n",
            "| epoch   4 |   720/ 2134 batches | loss     1.61\n",
            "| epoch   4 |   730/ 2134 batches | loss     1.42\n",
            "| epoch   4 |   740/ 2134 batches | loss     1.78\n",
            "| epoch   4 |   750/ 2134 batches | loss     1.84\n",
            "| epoch   4 |   760/ 2134 batches | loss     2.00\n",
            "| epoch   4 |   770/ 2134 batches | loss     1.74\n",
            "| epoch   4 |   780/ 2134 batches | loss     2.29\n",
            "| epoch   4 |   790/ 2134 batches | loss     1.51\n",
            "| epoch   4 |   800/ 2134 batches | loss     2.05\n",
            "| epoch   4 |   810/ 2134 batches | loss     1.56\n",
            "| epoch   4 |   820/ 2134 batches | loss     2.05\n",
            "| epoch   4 |   830/ 2134 batches | loss     2.71\n",
            "| epoch   4 |   840/ 2134 batches | loss     1.85\n",
            "| epoch   4 |   850/ 2134 batches | loss     1.77\n",
            "| epoch   4 |   860/ 2134 batches | loss     1.70\n",
            "| epoch   4 |   870/ 2134 batches | loss     2.16\n",
            "| epoch   4 |   880/ 2134 batches | loss     1.64\n",
            "| epoch   4 |   890/ 2134 batches | loss     1.72\n",
            "| epoch   4 |   900/ 2134 batches | loss     1.80\n",
            "| epoch   4 |   910/ 2134 batches | loss     1.72\n",
            "| epoch   4 |   920/ 2134 batches | loss     2.00\n",
            "| epoch   4 |   930/ 2134 batches | loss     2.30\n",
            "| epoch   4 |   940/ 2134 batches | loss     1.75\n",
            "| epoch   4 |   950/ 2134 batches | loss     1.21\n",
            "| epoch   4 |   960/ 2134 batches | loss     1.80\n",
            "| epoch   4 |   970/ 2134 batches | loss     1.55\n",
            "| epoch   4 |   980/ 2134 batches | loss     2.18\n",
            "| epoch   4 |   990/ 2134 batches | loss     1.56\n",
            "| epoch   4 |  1000/ 2134 batches | loss     2.24\n",
            "| epoch   4 |  1010/ 2134 batches | loss     2.35\n",
            "| epoch   4 |  1020/ 2134 batches | loss     2.09\n",
            "| epoch   4 |  1030/ 2134 batches | loss     2.05\n",
            "| epoch   4 |  1040/ 2134 batches | loss     1.91\n",
            "| epoch   4 |  1050/ 2134 batches | loss     1.62\n",
            "| epoch   4 |  1060/ 2134 batches | loss     1.57\n",
            "| epoch   4 |  1070/ 2134 batches | loss     1.55\n",
            "| epoch   4 |  1080/ 2134 batches | loss     1.71\n",
            "| epoch   4 |  1090/ 2134 batches | loss     2.42\n",
            "| epoch   4 |  1100/ 2134 batches | loss     1.82\n",
            "| epoch   4 |  1110/ 2134 batches | loss     2.00\n",
            "| epoch   4 |  1120/ 2134 batches | loss     1.96\n",
            "| epoch   4 |  1130/ 2134 batches | loss     1.83\n",
            "| epoch   4 |  1140/ 2134 batches | loss     2.24\n",
            "| epoch   4 |  1150/ 2134 batches | loss     1.80\n",
            "| epoch   4 |  1160/ 2134 batches | loss     1.51\n",
            "| epoch   4 |  1170/ 2134 batches | loss     2.02\n",
            "| epoch   4 |  1180/ 2134 batches | loss     1.93\n",
            "| epoch   4 |  1190/ 2134 batches | loss     1.72\n",
            "| epoch   4 |  1200/ 2134 batches | loss     1.83\n",
            "| epoch   4 |  1210/ 2134 batches | loss     1.41\n",
            "| epoch   4 |  1220/ 2134 batches | loss     2.15\n",
            "| epoch   4 |  1230/ 2134 batches | loss     2.04\n",
            "| epoch   4 |  1240/ 2134 batches | loss     2.26\n",
            "| epoch   4 |  1250/ 2134 batches | loss     1.91\n",
            "| epoch   4 |  1260/ 2134 batches | loss     1.93\n",
            "| epoch   4 |  1270/ 2134 batches | loss     1.45\n",
            "| epoch   4 |  1280/ 2134 batches | loss     1.99\n",
            "| epoch   4 |  1290/ 2134 batches | loss     1.20\n",
            "| epoch   4 |  1300/ 2134 batches | loss     2.14\n",
            "| epoch   4 |  1310/ 2134 batches | loss     2.05\n",
            "| epoch   4 |  1320/ 2134 batches | loss     1.41\n",
            "| epoch   4 |  1330/ 2134 batches | loss     1.74\n",
            "| epoch   4 |  1340/ 2134 batches | loss     2.29\n",
            "| epoch   4 |  1350/ 2134 batches | loss     1.69\n",
            "| epoch   4 |  1360/ 2134 batches | loss     1.59\n",
            "| epoch   4 |  1370/ 2134 batches | loss     1.67\n",
            "| epoch   4 |  1380/ 2134 batches | loss     2.05\n",
            "| epoch   4 |  1390/ 2134 batches | loss     1.50\n",
            "| epoch   4 |  1400/ 2134 batches | loss     1.90\n",
            "| epoch   4 |  1410/ 2134 batches | loss     1.73\n",
            "| epoch   4 |  1420/ 2134 batches | loss     1.46\n",
            "| epoch   4 |  1430/ 2134 batches | loss     1.66\n",
            "| epoch   4 |  1440/ 2134 batches | loss     2.23\n",
            "| epoch   4 |  1450/ 2134 batches | loss     1.82\n",
            "| epoch   4 |  1460/ 2134 batches | loss     1.84\n",
            "| epoch   4 |  1470/ 2134 batches | loss     2.14\n",
            "| epoch   4 |  1480/ 2134 batches | loss     1.66\n",
            "| epoch   4 |  1490/ 2134 batches | loss     1.76\n",
            "| epoch   4 |  1500/ 2134 batches | loss     1.93\n",
            "| epoch   4 |  1510/ 2134 batches | loss     1.63\n",
            "| epoch   4 |  1520/ 2134 batches | loss     1.75\n",
            "| epoch   4 |  1530/ 2134 batches | loss     1.69\n",
            "| epoch   4 |  1540/ 2134 batches | loss     1.63\n",
            "| epoch   4 |  1550/ 2134 batches | loss     2.00\n",
            "| epoch   4 |  1560/ 2134 batches | loss     2.06\n",
            "| epoch   4 |  1570/ 2134 batches | loss     1.51\n",
            "| epoch   4 |  1580/ 2134 batches | loss     1.81\n",
            "| epoch   4 |  1590/ 2134 batches | loss     2.08\n",
            "| epoch   4 |  1600/ 2134 batches | loss     1.82\n",
            "| epoch   4 |  1610/ 2134 batches | loss     1.67\n",
            "| epoch   4 |  1620/ 2134 batches | loss     2.20\n",
            "| epoch   4 |  1630/ 2134 batches | loss     1.62\n",
            "| epoch   4 |  1640/ 2134 batches | loss     1.63\n",
            "| epoch   4 |  1650/ 2134 batches | loss     1.82\n",
            "| epoch   4 |  1660/ 2134 batches | loss     1.89\n",
            "| epoch   4 |  1670/ 2134 batches | loss     1.70\n",
            "| epoch   4 |  1680/ 2134 batches | loss     1.46\n",
            "| epoch   4 |  1690/ 2134 batches | loss     1.94\n",
            "| epoch   4 |  1700/ 2134 batches | loss     2.08\n",
            "| epoch   4 |  1710/ 2134 batches | loss     1.65\n",
            "| epoch   4 |  1720/ 2134 batches | loss     2.02\n",
            "| epoch   4 |  1730/ 2134 batches | loss     1.53\n",
            "| epoch   4 |  1740/ 2134 batches | loss     1.71\n",
            "| epoch   4 |  1750/ 2134 batches | loss     2.24\n",
            "| epoch   4 |  1760/ 2134 batches | loss     2.03\n",
            "| epoch   4 |  1770/ 2134 batches | loss     1.99\n",
            "| epoch   4 |  1780/ 2134 batches | loss     1.99\n",
            "| epoch   4 |  1790/ 2134 batches | loss     1.87\n",
            "| epoch   4 |  1800/ 2134 batches | loss     1.92\n",
            "| epoch   4 |  1810/ 2134 batches | loss     1.69\n",
            "| epoch   4 |  1820/ 2134 batches | loss     1.88\n",
            "| epoch   4 |  1830/ 2134 batches | loss     1.86\n",
            "| epoch   4 |  1840/ 2134 batches | loss     1.58\n",
            "| epoch   4 |  1850/ 2134 batches | loss     2.03\n",
            "| epoch   4 |  1860/ 2134 batches | loss     2.23\n",
            "| epoch   4 |  1870/ 2134 batches | loss     1.74\n",
            "| epoch   4 |  1880/ 2134 batches | loss     2.17\n",
            "| epoch   4 |  1890/ 2134 batches | loss     2.11\n",
            "| epoch   4 |  1900/ 2134 batches | loss     1.96\n",
            "| epoch   4 |  1910/ 2134 batches | loss     1.54\n",
            "| epoch   4 |  1920/ 2134 batches | loss     1.65\n",
            "| epoch   4 |  1930/ 2134 batches | loss     1.82\n",
            "| epoch   4 |  1940/ 2134 batches | loss     1.95\n",
            "| epoch   4 |  1950/ 2134 batches | loss     1.78\n",
            "| epoch   4 |  1960/ 2134 batches | loss     1.92\n",
            "| epoch   4 |  1970/ 2134 batches | loss     1.43\n",
            "| epoch   4 |  1980/ 2134 batches | loss     1.67\n",
            "| epoch   4 |  1990/ 2134 batches | loss     2.03\n",
            "| epoch   4 |  2000/ 2134 batches | loss     1.91\n",
            "| epoch   4 |  2010/ 2134 batches | loss     1.58\n",
            "| epoch   4 |  2020/ 2134 batches | loss     1.33\n",
            "| epoch   4 |  2030/ 2134 batches | loss     1.63\n",
            "| epoch   4 |  2040/ 2134 batches | loss     1.65\n",
            "| epoch   4 |  2050/ 2134 batches | loss     1.90\n",
            "| epoch   4 |  2060/ 2134 batches | loss     1.65\n",
            "| epoch   4 |  2070/ 2134 batches | loss     2.13\n",
            "| epoch   4 |  2080/ 2134 batches | loss     2.18\n",
            "| epoch   4 |  2090/ 2134 batches | loss     1.71\n",
            "| epoch   4 |  2100/ 2134 batches | loss     1.35\n",
            "| epoch   4 |  2110/ 2134 batches | loss     1.70\n",
            "| epoch   4 |  2120/ 2134 batches | loss     1.46\n",
            "| epoch   4 |  2130/ 2134 batches | loss     1.50\n",
            "Epoch: 04 | Time: 8m 35s\n",
            "\tTrain Loss: 1.848 | Train PPL:   6.347\n",
            "\t Val. Loss: 2.741 |  Val. PPL:  15.506\n",
            "| epoch   5 |    10/ 2134 batches | loss     1.97\n",
            "| epoch   5 |    20/ 2134 batches | loss     1.43\n",
            "| epoch   5 |    30/ 2134 batches | loss     1.94\n",
            "| epoch   5 |    40/ 2134 batches | loss     2.00\n",
            "| epoch   5 |    50/ 2134 batches | loss     1.64\n",
            "| epoch   5 |    60/ 2134 batches | loss     2.20\n",
            "| epoch   5 |    70/ 2134 batches | loss     1.55\n",
            "| epoch   5 |    80/ 2134 batches | loss     2.76\n",
            "| epoch   5 |    90/ 2134 batches | loss     1.65\n",
            "| epoch   5 |   100/ 2134 batches | loss     2.24\n",
            "| epoch   5 |   110/ 2134 batches | loss     2.02\n",
            "| epoch   5 |   120/ 2134 batches | loss     2.09\n",
            "| epoch   5 |   130/ 2134 batches | loss     1.98\n",
            "| epoch   5 |   140/ 2134 batches | loss     2.40\n",
            "| epoch   5 |   150/ 2134 batches | loss     2.05\n",
            "| epoch   5 |   160/ 2134 batches | loss     2.32\n",
            "| epoch   5 |   170/ 2134 batches | loss     1.27\n",
            "| epoch   5 |   180/ 2134 batches | loss     1.88\n",
            "| epoch   5 |   190/ 2134 batches | loss     1.95\n",
            "| epoch   5 |   200/ 2134 batches | loss     2.34\n",
            "| epoch   5 |   210/ 2134 batches | loss     1.92\n",
            "| epoch   5 |   220/ 2134 batches | loss     1.90\n",
            "| epoch   5 |   230/ 2134 batches | loss     1.77\n",
            "| epoch   5 |   240/ 2134 batches | loss     1.53\n",
            "| epoch   5 |   250/ 2134 batches | loss     1.94\n",
            "| epoch   5 |   260/ 2134 batches | loss     2.07\n",
            "| epoch   5 |   270/ 2134 batches | loss     2.38\n",
            "| epoch   5 |   280/ 2134 batches | loss     1.74\n",
            "| epoch   5 |   290/ 2134 batches | loss     1.95\n",
            "| epoch   5 |   300/ 2134 batches | loss     2.15\n",
            "| epoch   5 |   310/ 2134 batches | loss     1.99\n",
            "| epoch   5 |   320/ 2134 batches | loss     1.37\n",
            "| epoch   5 |   330/ 2134 batches | loss     1.59\n",
            "| epoch   5 |   340/ 2134 batches | loss     1.80\n",
            "| epoch   5 |   350/ 2134 batches | loss     1.42\n",
            "| epoch   5 |   360/ 2134 batches | loss     1.96\n",
            "| epoch   5 |   370/ 2134 batches | loss     2.07\n",
            "| epoch   5 |   380/ 2134 batches | loss     1.81\n",
            "| epoch   5 |   390/ 2134 batches | loss     1.84\n",
            "| epoch   5 |   400/ 2134 batches | loss     1.53\n",
            "| epoch   5 |   410/ 2134 batches | loss     1.90\n",
            "| epoch   5 |   420/ 2134 batches | loss     1.84\n",
            "| epoch   5 |   430/ 2134 batches | loss     1.76\n",
            "| epoch   5 |   440/ 2134 batches | loss     1.93\n",
            "| epoch   5 |   450/ 2134 batches | loss     1.93\n",
            "| epoch   5 |   460/ 2134 batches | loss     1.86\n",
            "| epoch   5 |   470/ 2134 batches | loss     1.76\n",
            "| epoch   5 |   480/ 2134 batches | loss     1.85\n",
            "| epoch   5 |   490/ 2134 batches | loss     1.91\n",
            "| epoch   5 |   500/ 2134 batches | loss     1.78\n",
            "| epoch   5 |   510/ 2134 batches | loss     2.08\n",
            "| epoch   5 |   520/ 2134 batches | loss     2.13\n",
            "| epoch   5 |   530/ 2134 batches | loss     1.28\n",
            "| epoch   5 |   540/ 2134 batches | loss     1.33\n",
            "| epoch   5 |   550/ 2134 batches | loss     1.89\n",
            "| epoch   5 |   560/ 2134 batches | loss     1.24\n",
            "| epoch   5 |   570/ 2134 batches | loss     1.96\n",
            "| epoch   5 |   580/ 2134 batches | loss     1.99\n",
            "| epoch   5 |   590/ 2134 batches | loss     2.19\n",
            "| epoch   5 |   600/ 2134 batches | loss     1.96\n",
            "| epoch   5 |   610/ 2134 batches | loss     2.12\n",
            "| epoch   5 |   620/ 2134 batches | loss     1.71\n",
            "| epoch   5 |   630/ 2134 batches | loss     2.06\n",
            "| epoch   5 |   640/ 2134 batches | loss     1.80\n",
            "| epoch   5 |   650/ 2134 batches | loss     2.49\n",
            "| epoch   5 |   660/ 2134 batches | loss     1.92\n",
            "| epoch   5 |   670/ 2134 batches | loss     1.98\n",
            "| epoch   5 |   680/ 2134 batches | loss     1.59\n",
            "| epoch   5 |   690/ 2134 batches | loss     1.66\n",
            "| epoch   5 |   700/ 2134 batches | loss     2.19\n",
            "| epoch   5 |   710/ 2134 batches | loss     2.03\n",
            "| epoch   5 |   720/ 2134 batches | loss     2.02\n",
            "| epoch   5 |   730/ 2134 batches | loss     1.69\n",
            "| epoch   5 |   740/ 2134 batches | loss     2.01\n",
            "| epoch   5 |   750/ 2134 batches | loss     1.99\n",
            "| epoch   5 |   760/ 2134 batches | loss     1.38\n",
            "| epoch   5 |   770/ 2134 batches | loss     1.95\n",
            "| epoch   5 |   780/ 2134 batches | loss     1.72\n",
            "| epoch   5 |   790/ 2134 batches | loss     1.67\n",
            "| epoch   5 |   800/ 2134 batches | loss     2.00\n",
            "| epoch   5 |   810/ 2134 batches | loss     1.82\n",
            "| epoch   5 |   820/ 2134 batches | loss     1.66\n",
            "| epoch   5 |   830/ 2134 batches | loss     2.10\n",
            "| epoch   5 |   840/ 2134 batches | loss     1.70\n",
            "| epoch   5 |   850/ 2134 batches | loss     2.19\n",
            "| epoch   5 |   860/ 2134 batches | loss     2.08\n",
            "| epoch   5 |   870/ 2134 batches | loss     1.80\n",
            "| epoch   5 |   880/ 2134 batches | loss     2.34\n",
            "| epoch   5 |   890/ 2134 batches | loss     1.62\n",
            "| epoch   5 |   900/ 2134 batches | loss     2.23\n",
            "| epoch   5 |   910/ 2134 batches | loss     1.88\n",
            "| epoch   5 |   920/ 2134 batches | loss     1.70\n",
            "| epoch   5 |   930/ 2134 batches | loss     1.80\n",
            "| epoch   5 |   940/ 2134 batches | loss     2.30\n",
            "| epoch   5 |   950/ 2134 batches | loss     2.10\n",
            "| epoch   5 |   960/ 2134 batches | loss     2.54\n",
            "| epoch   5 |   970/ 2134 batches | loss     1.78\n",
            "| epoch   5 |   980/ 2134 batches | loss     1.50\n",
            "| epoch   5 |   990/ 2134 batches | loss     1.90\n",
            "| epoch   5 |  1000/ 2134 batches | loss     1.84\n",
            "| epoch   5 |  1010/ 2134 batches | loss     1.97\n",
            "| epoch   5 |  1020/ 2134 batches | loss     1.70\n",
            "| epoch   5 |  1030/ 2134 batches | loss     1.50\n",
            "| epoch   5 |  1040/ 2134 batches | loss     1.77\n",
            "| epoch   5 |  1050/ 2134 batches | loss     2.00\n",
            "| epoch   5 |  1060/ 2134 batches | loss     2.02\n",
            "| epoch   5 |  1070/ 2134 batches | loss     1.58\n",
            "| epoch   5 |  1080/ 2134 batches | loss     2.19\n",
            "| epoch   5 |  1090/ 2134 batches | loss     1.88\n",
            "| epoch   5 |  1100/ 2134 batches | loss     1.91\n",
            "| epoch   5 |  1110/ 2134 batches | loss     1.55\n",
            "| epoch   5 |  1120/ 2134 batches | loss     2.22\n",
            "| epoch   5 |  1130/ 2134 batches | loss     1.97\n",
            "| epoch   5 |  1140/ 2134 batches | loss     1.81\n",
            "| epoch   5 |  1150/ 2134 batches | loss     1.58\n",
            "| epoch   5 |  1160/ 2134 batches | loss     1.71\n",
            "| epoch   5 |  1170/ 2134 batches | loss     1.86\n",
            "| epoch   5 |  1180/ 2134 batches | loss     1.58\n",
            "| epoch   5 |  1190/ 2134 batches | loss     1.91\n",
            "| epoch   5 |  1200/ 2134 batches | loss     2.09\n",
            "| epoch   5 |  1210/ 2134 batches | loss     1.80\n",
            "| epoch   5 |  1220/ 2134 batches | loss     2.21\n",
            "| epoch   5 |  1230/ 2134 batches | loss     1.64\n",
            "| epoch   5 |  1240/ 2134 batches | loss     1.72\n",
            "| epoch   5 |  1250/ 2134 batches | loss     1.84\n",
            "| epoch   5 |  1260/ 2134 batches | loss     1.66\n",
            "| epoch   5 |  1270/ 2134 batches | loss     1.72\n",
            "| epoch   5 |  1280/ 2134 batches | loss     1.75\n",
            "| epoch   5 |  1290/ 2134 batches | loss     1.83\n",
            "| epoch   5 |  1300/ 2134 batches | loss     2.39\n",
            "| epoch   5 |  1310/ 2134 batches | loss     1.67\n",
            "| epoch   5 |  1320/ 2134 batches | loss     2.12\n",
            "| epoch   5 |  1330/ 2134 batches | loss     1.98\n",
            "| epoch   5 |  1340/ 2134 batches | loss     1.90\n",
            "| epoch   5 |  1350/ 2134 batches | loss     1.69\n",
            "| epoch   5 |  1360/ 2134 batches | loss     2.02\n",
            "| epoch   5 |  1370/ 2134 batches | loss     1.94\n",
            "| epoch   5 |  1380/ 2134 batches | loss     1.50\n",
            "| epoch   5 |  1390/ 2134 batches | loss     1.42\n",
            "| epoch   5 |  1400/ 2134 batches | loss     1.81\n",
            "| epoch   5 |  1410/ 2134 batches | loss     1.67\n",
            "| epoch   5 |  1420/ 2134 batches | loss     2.16\n",
            "| epoch   5 |  1430/ 2134 batches | loss     2.07\n",
            "| epoch   5 |  1440/ 2134 batches | loss     1.46\n",
            "| epoch   5 |  1450/ 2134 batches | loss     1.56\n",
            "| epoch   5 |  1460/ 2134 batches | loss     2.18\n",
            "| epoch   5 |  1470/ 2134 batches | loss     1.75\n",
            "| epoch   5 |  1480/ 2134 batches | loss     1.76\n",
            "| epoch   5 |  1490/ 2134 batches | loss     2.15\n",
            "| epoch   5 |  1500/ 2134 batches | loss     1.70\n",
            "| epoch   5 |  1510/ 2134 batches | loss     2.10\n",
            "| epoch   5 |  1520/ 2134 batches | loss     1.39\n",
            "| epoch   5 |  1530/ 2134 batches | loss     1.80\n",
            "| epoch   5 |  1540/ 2134 batches | loss     1.78\n",
            "| epoch   5 |  1550/ 2134 batches | loss     1.85\n",
            "| epoch   5 |  1560/ 2134 batches | loss     1.87\n",
            "| epoch   5 |  1570/ 2134 batches | loss     1.86\n",
            "| epoch   5 |  1580/ 2134 batches | loss     2.33\n",
            "| epoch   5 |  1590/ 2134 batches | loss     1.93\n",
            "| epoch   5 |  1600/ 2134 batches | loss     2.15\n",
            "| epoch   5 |  1610/ 2134 batches | loss     1.25\n",
            "| epoch   5 |  1620/ 2134 batches | loss     2.03\n",
            "| epoch   5 |  1630/ 2134 batches | loss     1.87\n",
            "| epoch   5 |  1640/ 2134 batches | loss     1.92\n",
            "| epoch   5 |  1650/ 2134 batches | loss     1.71\n",
            "| epoch   5 |  1660/ 2134 batches | loss     1.88\n",
            "| epoch   5 |  1670/ 2134 batches | loss     1.76\n",
            "| epoch   5 |  1680/ 2134 batches | loss     2.30\n",
            "| epoch   5 |  1690/ 2134 batches | loss     2.03\n",
            "| epoch   5 |  1700/ 2134 batches | loss     2.23\n",
            "| epoch   5 |  1710/ 2134 batches | loss     1.83\n",
            "| epoch   5 |  1720/ 2134 batches | loss     1.69\n",
            "| epoch   5 |  1730/ 2134 batches | loss     1.64\n",
            "| epoch   5 |  1740/ 2134 batches | loss     1.72\n",
            "| epoch   5 |  1750/ 2134 batches | loss     1.76\n",
            "| epoch   5 |  1760/ 2134 batches | loss     1.87\n",
            "| epoch   5 |  1770/ 2134 batches | loss     2.10\n",
            "| epoch   5 |  1780/ 2134 batches | loss     2.09\n",
            "| epoch   5 |  1790/ 2134 batches | loss     1.57\n",
            "| epoch   5 |  1800/ 2134 batches | loss     2.02\n",
            "| epoch   5 |  1810/ 2134 batches | loss     2.16\n",
            "| epoch   5 |  1820/ 2134 batches | loss     1.77\n",
            "| epoch   5 |  1830/ 2134 batches | loss     1.59\n",
            "| epoch   5 |  1840/ 2134 batches | loss     1.68\n",
            "| epoch   5 |  1850/ 2134 batches | loss     1.49\n",
            "| epoch   5 |  1860/ 2134 batches | loss     1.65\n",
            "| epoch   5 |  1870/ 2134 batches | loss     1.90\n",
            "| epoch   5 |  1880/ 2134 batches | loss     1.74\n",
            "| epoch   5 |  1890/ 2134 batches | loss     1.99\n",
            "| epoch   5 |  1900/ 2134 batches | loss     1.43\n",
            "| epoch   5 |  1910/ 2134 batches | loss     1.49\n",
            "| epoch   5 |  1920/ 2134 batches | loss     1.34\n",
            "| epoch   5 |  1930/ 2134 batches | loss     1.77\n",
            "| epoch   5 |  1940/ 2134 batches | loss     1.44\n",
            "| epoch   5 |  1950/ 2134 batches | loss     1.44\n",
            "| epoch   5 |  1960/ 2134 batches | loss     1.42\n",
            "| epoch   5 |  1970/ 2134 batches | loss     1.89\n",
            "| epoch   5 |  1980/ 2134 batches | loss     1.45\n",
            "| epoch   5 |  1990/ 2134 batches | loss     1.76\n",
            "| epoch   5 |  2000/ 2134 batches | loss     2.02\n",
            "| epoch   5 |  2010/ 2134 batches | loss     1.88\n",
            "| epoch   5 |  2020/ 2134 batches | loss     1.88\n",
            "| epoch   5 |  2030/ 2134 batches | loss     2.08\n",
            "| epoch   5 |  2040/ 2134 batches | loss     1.91\n",
            "| epoch   5 |  2050/ 2134 batches | loss     1.72\n",
            "| epoch   5 |  2060/ 2134 batches | loss     1.67\n",
            "| epoch   5 |  2070/ 2134 batches | loss     1.75\n",
            "| epoch   5 |  2080/ 2134 batches | loss     1.96\n",
            "| epoch   5 |  2090/ 2134 batches | loss     1.54\n",
            "| epoch   5 |  2100/ 2134 batches | loss     2.17\n",
            "| epoch   5 |  2110/ 2134 batches | loss     1.48\n",
            "| epoch   5 |  2120/ 2134 batches | loss     2.03\n",
            "| epoch   5 |  2130/ 2134 batches | loss     1.72\n",
            "Epoch: 05 | Time: 8m 46s\n",
            "\tTrain Loss: 1.822 | Train PPL:   6.184\n",
            "\t Val. Loss: 2.853 |  Val. PPL:  17.335\n",
            "| epoch   6 |    10/ 2134 batches | loss     1.59\n",
            "| epoch   6 |    20/ 2134 batches | loss     1.73\n",
            "| epoch   6 |    30/ 2134 batches | loss     2.16\n",
            "| epoch   6 |    40/ 2134 batches | loss     1.76\n",
            "| epoch   6 |    50/ 2134 batches | loss     1.67\n",
            "| epoch   6 |    60/ 2134 batches | loss     1.91\n",
            "| epoch   6 |    70/ 2134 batches | loss     1.46\n",
            "| epoch   6 |    80/ 2134 batches | loss     1.18\n",
            "| epoch   6 |    90/ 2134 batches | loss     1.82\n",
            "| epoch   6 |   100/ 2134 batches | loss     1.93\n",
            "| epoch   6 |   110/ 2134 batches | loss     1.48\n",
            "| epoch   6 |   120/ 2134 batches | loss     1.70\n",
            "| epoch   6 |   130/ 2134 batches | loss     1.69\n",
            "| epoch   6 |   140/ 2134 batches | loss     2.06\n",
            "| epoch   6 |   150/ 2134 batches | loss     1.62\n",
            "| epoch   6 |   160/ 2134 batches | loss     1.91\n",
            "| epoch   6 |   170/ 2134 batches | loss     1.89\n",
            "| epoch   6 |   180/ 2134 batches | loss     1.56\n",
            "| epoch   6 |   190/ 2134 batches | loss     2.34\n",
            "| epoch   6 |   200/ 2134 batches | loss     1.84\n",
            "| epoch   6 |   210/ 2134 batches | loss     2.07\n",
            "| epoch   6 |   220/ 2134 batches | loss     1.62\n",
            "| epoch   6 |   230/ 2134 batches | loss     1.79\n",
            "| epoch   6 |   240/ 2134 batches | loss     1.53\n",
            "| epoch   6 |   250/ 2134 batches | loss     1.57\n",
            "| epoch   6 |   260/ 2134 batches | loss     1.98\n",
            "| epoch   6 |   270/ 2134 batches | loss     1.68\n",
            "| epoch   6 |   280/ 2134 batches | loss     2.20\n",
            "| epoch   6 |   290/ 2134 batches | loss     2.05\n",
            "| epoch   6 |   300/ 2134 batches | loss     1.66\n",
            "| epoch   6 |   310/ 2134 batches | loss     1.54\n",
            "| epoch   6 |   320/ 2134 batches | loss     1.99\n",
            "| epoch   6 |   330/ 2134 batches | loss     2.38\n",
            "| epoch   6 |   340/ 2134 batches | loss     2.07\n",
            "| epoch   6 |   350/ 2134 batches | loss     1.52\n",
            "| epoch   6 |   360/ 2134 batches | loss     2.08\n",
            "| epoch   6 |   370/ 2134 batches | loss     1.68\n",
            "| epoch   6 |   380/ 2134 batches | loss     1.49\n",
            "| epoch   6 |   390/ 2134 batches | loss     1.83\n",
            "| epoch   6 |   400/ 2134 batches | loss     2.14\n",
            "| epoch   6 |   410/ 2134 batches | loss     1.82\n",
            "| epoch   6 |   420/ 2134 batches | loss     2.00\n",
            "| epoch   6 |   430/ 2134 batches | loss     1.32\n",
            "| epoch   6 |   440/ 2134 batches | loss     1.39\n",
            "| epoch   6 |   450/ 2134 batches | loss     1.71\n",
            "| epoch   6 |   460/ 2134 batches | loss     1.78\n",
            "| epoch   6 |   470/ 2134 batches | loss     2.00\n",
            "| epoch   6 |   480/ 2134 batches | loss     1.71\n",
            "| epoch   6 |   490/ 2134 batches | loss     1.92\n",
            "| epoch   6 |   500/ 2134 batches | loss     1.68\n",
            "| epoch   6 |   510/ 2134 batches | loss     2.28\n",
            "| epoch   6 |   520/ 2134 batches | loss     1.77\n",
            "| epoch   6 |   530/ 2134 batches | loss     1.88\n",
            "| epoch   6 |   540/ 2134 batches | loss     1.65\n",
            "| epoch   6 |   550/ 2134 batches | loss     1.50\n",
            "| epoch   6 |   560/ 2134 batches | loss     2.19\n",
            "| epoch   6 |   570/ 2134 batches | loss     2.31\n",
            "| epoch   6 |   580/ 2134 batches | loss     1.62\n",
            "| epoch   6 |   590/ 2134 batches | loss     1.58\n",
            "| epoch   6 |   600/ 2134 batches | loss     2.29\n",
            "| epoch   6 |   610/ 2134 batches | loss     1.90\n",
            "| epoch   6 |   620/ 2134 batches | loss     1.98\n",
            "| epoch   6 |   630/ 2134 batches | loss     1.79\n",
            "| epoch   6 |   640/ 2134 batches | loss     1.47\n",
            "| epoch   6 |   650/ 2134 batches | loss     1.21\n",
            "| epoch   6 |   660/ 2134 batches | loss     1.62\n",
            "| epoch   6 |   670/ 2134 batches | loss     1.93\n",
            "| epoch   6 |   680/ 2134 batches | loss     1.74\n",
            "| epoch   6 |   690/ 2134 batches | loss     1.67\n",
            "| epoch   6 |   700/ 2134 batches | loss     1.54\n",
            "| epoch   6 |   710/ 2134 batches | loss     2.02\n",
            "| epoch   6 |   720/ 2134 batches | loss     1.90\n",
            "| epoch   6 |   730/ 2134 batches | loss     1.98\n",
            "| epoch   6 |   740/ 2134 batches | loss     1.56\n",
            "| epoch   6 |   750/ 2134 batches | loss     2.02\n",
            "| epoch   6 |   760/ 2134 batches | loss     1.61\n",
            "| epoch   6 |   770/ 2134 batches | loss     1.51\n",
            "| epoch   6 |   780/ 2134 batches | loss     1.75\n",
            "| epoch   6 |   790/ 2134 batches | loss     2.14\n",
            "| epoch   6 |   800/ 2134 batches | loss     1.67\n",
            "| epoch   6 |   810/ 2134 batches | loss     1.66\n",
            "| epoch   6 |   820/ 2134 batches | loss     1.39\n",
            "| epoch   6 |   830/ 2134 batches | loss     1.92\n",
            "| epoch   6 |   840/ 2134 batches | loss     1.85\n",
            "| epoch   6 |   850/ 2134 batches | loss     1.68\n",
            "| epoch   6 |   860/ 2134 batches | loss     1.50\n",
            "| epoch   6 |   870/ 2134 batches | loss     1.54\n",
            "| epoch   6 |   880/ 2134 batches | loss     1.66\n",
            "| epoch   6 |   890/ 2134 batches | loss     1.97\n",
            "| epoch   6 |   900/ 2134 batches | loss     1.81\n",
            "| epoch   6 |   910/ 2134 batches | loss     1.62\n",
            "| epoch   6 |   920/ 2134 batches | loss     1.21\n",
            "| epoch   6 |   930/ 2134 batches | loss     2.19\n",
            "| epoch   6 |   940/ 2134 batches | loss     1.86\n",
            "| epoch   6 |   950/ 2134 batches | loss     2.09\n",
            "| epoch   6 |   960/ 2134 batches | loss     1.96\n",
            "| epoch   6 |   970/ 2134 batches | loss     1.85\n",
            "| epoch   6 |   980/ 2134 batches | loss     2.03\n",
            "| epoch   6 |   990/ 2134 batches | loss     1.86\n",
            "| epoch   6 |  1000/ 2134 batches | loss     2.05\n",
            "| epoch   6 |  1010/ 2134 batches | loss     1.73\n",
            "| epoch   6 |  1020/ 2134 batches | loss     1.99\n",
            "| epoch   6 |  1030/ 2134 batches | loss     1.80\n",
            "| epoch   6 |  1040/ 2134 batches | loss     1.80\n",
            "| epoch   6 |  1050/ 2134 batches | loss     1.57\n",
            "| epoch   6 |  1060/ 2134 batches | loss     1.75\n",
            "| epoch   6 |  1070/ 2134 batches | loss     1.62\n",
            "| epoch   6 |  1080/ 2134 batches | loss     1.64\n",
            "| epoch   6 |  1090/ 2134 batches | loss     1.95\n",
            "| epoch   6 |  1100/ 2134 batches | loss     1.60\n",
            "| epoch   6 |  1110/ 2134 batches | loss     1.88\n",
            "| epoch   6 |  1120/ 2134 batches | loss     1.85\n",
            "| epoch   6 |  1130/ 2134 batches | loss     1.92\n",
            "| epoch   6 |  1140/ 2134 batches | loss     1.95\n",
            "| epoch   6 |  1150/ 2134 batches | loss     2.31\n",
            "| epoch   6 |  1160/ 2134 batches | loss     1.69\n",
            "| epoch   6 |  1170/ 2134 batches | loss     1.55\n",
            "| epoch   6 |  1180/ 2134 batches | loss     1.70\n",
            "| epoch   6 |  1190/ 2134 batches | loss     1.26\n",
            "| epoch   6 |  1200/ 2134 batches | loss     1.73\n",
            "| epoch   6 |  1210/ 2134 batches | loss     2.07\n",
            "| epoch   6 |  1220/ 2134 batches | loss     1.83\n",
            "| epoch   6 |  1230/ 2134 batches | loss     2.10\n",
            "| epoch   6 |  1240/ 2134 batches | loss     2.33\n",
            "| epoch   6 |  1250/ 2134 batches | loss     1.49\n",
            "| epoch   6 |  1260/ 2134 batches | loss     1.51\n",
            "| epoch   6 |  1270/ 2134 batches | loss     1.91\n",
            "| epoch   6 |  1280/ 2134 batches | loss     2.12\n",
            "| epoch   6 |  1290/ 2134 batches | loss     1.58\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-08564f75809f>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-fea5a768275b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maa_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#smi = [smi len, batch size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-509d24172128>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, aa, aa_len, smi, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#insert input token embedding, previous hidden, all enconder hidden states, and mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m#receive output tensor (predictions) and new hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m#place predictions in a tensor holding predictions for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a28b5a1f82ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#embedded = [1, batch size, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#a = [batch size, src len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-851225c42ff6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#encoder_outputs = [batch size, src len, enc hid dim * 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#energy = [batch size, src len, dec hid dim] #nn.Linear(dec_hid_dim, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 54.56 MiB is free. Process 6801 has 39.51 GiB memory in use. Of the allocated memory 36.94 GiB is allocated by PyTorch, and 859.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import random\n",
        "import math\n",
        "\n",
        "N_EPOCHS = 6\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'cello-002-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we load the parameters from our best validation loss and get our results on the test set.\n"
      ],
      "metadata": {
        "id": "AdFZPUGQOyF0"
      },
      "id": "AdFZPUGQOyF0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0faf713d",
      "metadata": {
        "id": "0faf713d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0677fd12-e742-4b0b-b31a-1d9d22c6b41b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 2.685 | Test PPL:  14.662 |\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import random\n",
        "import math\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/CPI project/cello-002-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_dataloader, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Now we can use our trained model to generate SMILES code.\n",
        "\n",
        "**Note:** these translations will be poor compared to examples shown in paper as they use hidden dimension sizes of 1000 and train for 4 days! They have been cherry picked in order to show off what attention should look like on a sufficiently sized model.\n",
        "\n",
        "Our `generate_inhibitor` will do the following:\n",
        "- ensure our model is in evaluation mode, which it should always be for inference\n",
        "- tokenize the source amino acid sequence\n",
        "- numericalize the tokens\n",
        "- convert it to a tensor and add a batch dimension\n",
        "- get the length of the AA sequence and convert to a tensor\n",
        "- feed the sequence into the encoder\n",
        "- create the mask for the AA sequence\n",
        "- create a list to hold the output SMILES, initialized with an `<sos>` token\n",
        "- create a tensor to hold the attention values\n",
        "- while we have not hit a maximum length (50 here)\n",
        "  - get the input tensor, which should be either `<sos>` or the last predicted token\n",
        "  - feed the input, all encoder outputs, hidden state and mask into the decoder\n",
        "  - store attention values\n",
        "  - get the predicted next token\n",
        "  - add prediction to current output sentence prediction\n",
        "  - break if the prediction was an `<eos>` token\n",
        "- convert the output SMILES from indexes to tokens\n",
        "- return the output SMILES (with the `<sos>` token removed) and the attention values over the sequence"
      ],
      "metadata": {
        "id": "MRKgo3EiO1CU"
      },
      "id": "MRKgo3EiO1CU"
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "id": "a58c8fb5",
      "metadata": {
        "id": "a58c8fb5"
      },
      "outputs": [],
      "source": [
        "def generate_inhibitor(aa_seq, smi_seq):\n",
        "  tokenized = be(list(aa_seq)) #tokenize sentence\n",
        "  numericalized = aa_voc.lookup_indices(tokenized) #convert tokens into indexes\n",
        "  aa_len = torch.tensor(len(numericalized)).unsqueeze(0)\n",
        "  tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device) #convert to tensor and add batch dimension\n",
        "\n",
        "  with torch.no_grad():\n",
        "    encoder_outputs, hidden = model.encoder(tensor, aa_len)\n",
        "\n",
        "  mask = model.create_mask(tensor)\n",
        "  smiles_indexes = smiles_voc.lookup_indices(be(tokenize_smiles.tokenize(smi_seq)))\n",
        "  attentions = torch.zeros(50, 1, len(numericalized)).to(device)\n",
        "  smiles = []\n",
        "\n",
        "  for i in range(50):\n",
        "    smiles_tensor = torch.LongTensor([smiles_indexes[0]]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output, hidden, attention = model.decoder(smiles_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "    attentions[i] = attention\n",
        "    pred_token = output.argmax(1).item()\n",
        "    smiles.append(pred_token)\n",
        "\n",
        "    if pred_token == smiles_voc(['<eos>'])[0]:\n",
        "      break\n",
        "\n",
        "  smiles = smiles_voc.lookup_tokens(smiles) #we ignore the first token, just like we do in the training loop\n",
        "\n",
        "  return ''.join(smiles[1:]), attentions[:len(smiles)-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll make a function that displays the model's attention over the AA sequence for each target token generated."
      ],
      "metadata": {
        "id": "hNdbeEDKPhbY"
      },
      "id": "hNdbeEDKPhbY"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def display_attention(sentence, translation, attention):\n",
        "\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "\n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "\n",
        "    ax.tick_params(labelsize=5)\n",
        "\n",
        "    x_ticks = [''] + ['<sos>'] + [t for t in sentence] + ['<eos>']\n",
        "    y_ticks = [''] + list(translation)\n",
        "\n",
        "    ax.set_xticklabels(x_ticks, rotation=45)\n",
        "    ax.set_yticklabels(y_ticks)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "PH7NivoNysro"
      },
      "id": "PH7NivoNysro",
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example from the training dataset:\n",
        "\n",
        "- Generated inhibitor seems to have simple carbons repeated\n",
        "- Attention also shows only the last column(`<eos>`) was used for generating SMILES"
      ],
      "metadata": {
        "id": "XUHnFIMgPnQV"
      },
      "id": "XUHnFIMgPnQV"
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "id": "d39cf49a",
      "metadata": {
        "id": "d39cf49a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e83cf9-56af-44be-d1b6-493845eba249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amino acid sequence: MLSFQYPDVYRDETSVQDYHGHKICDPYAWLEDPDSEQTKAFVEAQNKITVPFLEQCPIRGLYKERMTELYDYPKYSCHFKKGKRYFYFYNTGLQNQRVLYVQDSLEGEARVFLDPNTLSDDGTVALRGYAFSEDGEYFAYGLSASGSDWVTIKFMKVDGAKELPDVLERVKFTCMAWTHDGKGMFYNSYPQQDGKSDGTETSTNLHQKLCYHVLGTDQSEDVLCAEFPDEPKWMGGAELSDDGRYVLLSIWEGCDPVNRLWYCDLQQGSNGINGILKWVKLIDNFEGEYDYITNEGTVFTFKTNRNSPNYRLINIDFTDPDESKWKVLVPEHEKDVLEWVACVRSNFLVLCYLRNVKNILQLHDLTTGALLKTFPLDVGSVVGYSGRKKDSEIFYQFTSFLSPGVIYHCDLTREELEPRVFREVTVKGIDASDYQTIQVFYPSKDGTKIPMFIVHKKGIKLDGSHPAFLYGYGGFNISITPNYSVSRLIFVRHMGGVLAVANIRGGGEYGETWHKGGILANKQNCFDDFQCAAEYLIKEGYTTSKRLTINGGSNGGLLVAACANQRPDLFGCVIAQVGVMDMLKFHKFTIGHAWTTDYGCSDSKQHFEWLLKYSPLHNVKLPEADDIQYPSMLLLTADHDDRVVPLHSLKFIATLQYIVGRSRKQSNPLLIHVDTKAGHGPGKPTAKVIEEVSDMFAFIARCLNIEWIQ\n",
            "710\n",
            "Inhibitor: O=C(C1CC(C1)c1ccccc1)N1C2CCC(CC2)[C@H]1C(=O)N1CCCC1\n"
          ]
        }
      ],
      "source": [
        "candidate = _train[8][2]\n",
        "candidate_inhibitor = _train[8][0]\n",
        "\n",
        "print(f'Amino acid sequence: {candidate}')\n",
        "print(len(candidate))\n",
        "print(f'Inhibitor: {candidate_inhibitor}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inhibitor, attention = generate_inhibitor(candidate, candidate_inhibitor)\n",
        "\n",
        "print(f'Generated inhibitor: {inhibitor}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uJHoA_4_yQq",
        "outputId": "eb44e066-6b69-42a5-c7b5-c2a0b639958f"
      },
      "id": "5uJHoA_4_yQq",
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated inhibitor: CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_attention(candidate, inhibitor, attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "izyKP4U4AeFr",
        "outputId": "e1a60887-3881-43b7-d623-57e9d3e4a5ed"
      },
      "id": "izyKP4U4AeFr",
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-300-8636fa165f0c>:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(x_ticks, rotation=45)\n",
            "<ipython-input-300-8636fa165f0c>:19: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels(y_ticks)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example using a simpler AA sequence\n",
        "\n",
        "- Shows that most of the information is being extracted from the last two AA seqences\n"
      ],
      "metadata": {
        "id": "VFYg76LuP8f0"
      },
      "id": "VFYg76LuP8f0"
    },
    {
      "cell_type": "code",
      "source": [
        "candidate = 'GGGHV'\n",
        "inhibitor, attention = generate_inhibitor(candidate, candidate_inhibitor)\n",
        "print(f'Generated inhibitor: {inhibitor}')\n",
        "display_attention(candidate, inhibitor, attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "4iFu9DrLVbzr",
        "outputId": "7a41e1af-a99d-45dc-f9b9-2b44165e5a73"
      },
      "id": "4iFu9DrLVbzr",
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated inhibitor: CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-300-8636fa165f0c>:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(x_ticks, rotation=45)\n",
            "<ipython-input-300-8636fa165f0c>:19: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels(y_ticks)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further improvements:\n",
        "- Better architecture : Transformer implementation (Self-, Multi-head attention)\n",
        "- Pretrained embedding : grouping amino acids with similar properties can help\n",
        "- Computational chemistry methods : Calculating actual physicochemical properties of proteins can help in prediction\n"
      ],
      "metadata": {
        "id": "mOresea3QMWp"
      },
      "id": "mOresea3QMWp"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}